# RAG Evaluation Test Suite

RAG(Retrieval-Augmented Generation) ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ì…ë‹ˆë‹¤.

## ğŸš€ Quick Start (ì „ì²´ ì›Œí¬í”Œë¡œìš°)

```bash
cd apps/server

# 1. HotpotQA ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (50ê°œ ìƒ˜í”Œ)
python tests/evaluation/prepare_datasets.py --dataset hotpotqa --samples 50

# 2. ë¬¸ì„œë¥¼ Knowledge Baseì— ì¸ë±ì‹±
python tests/evaluation/index_documents.py --dataset hotpotqa --kb-name "RAG Eval - HotpotQA"

# 3. ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (KB IDëŠ” 2ë²ˆì—ì„œ ì¶œë ¥ë¨)
python tests/evaluation/run_benchmark.py --kb-id YOUR_KB_ID --dataset hotpotqa --samples 50

# 4. ë¦¬í¬íŠ¸ í™•ì¸
cat tests/evaluation/reports/rag_eval_hotpotqa_*.md
```

## ğŸ“‚ êµ¬ì¡°

```
tests/evaluation/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ rag_metrics.py        # í‰ê°€ ì§€í‘œ êµ¬í˜„
â”œâ”€â”€ rag_evaluator.py      # í‰ê°€ í”„ë ˆì„ì›Œí¬
â”œâ”€â”€ test_rag_baseline.py  # ë² ì´ìŠ¤ë¼ì¸ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ sample_qa.json    # ìƒ˜í”Œ ë°ì´í„°ì…‹
â””â”€â”€ reports/              # í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥
```

## ğŸš€ ì‚¬ìš©ë²•

### 1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
cd apps/server
pytest tests/evaluation/test_rag_baseline.py -v
```

### 2. ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •

```python
from tests.evaluation.rag_evaluator import RAGEvaluator, EvaluationConfig, DatasetLoader
from tests.evaluation.rag_metrics import RetrievalResult

# ì„¤ì •
config = EvaluationConfig(
    dataset_name="baseline_v1",
    knowledge_base_id="your-kb-id",
    top_k_values=[1, 3, 5, 10]
)

# ìƒ˜í”Œ ë¡œë“œ
samples = DatasetLoader.load_json("tests/evaluation/datasets/sample_qa.json")

# í‰ê°€ ì‹¤í–‰
evaluator = RAGEvaluator(config)
results = evaluator.evaluate(samples, your_retrieval_func)

# ë¦¬í¬íŠ¸ ì €ì¥
evaluator.save_report(results)
print(results.summary())
```

### 3. ì„±ëŠ¥ ë¹„êµ

```python
# ë‘ ë¦¬í¬íŠ¸ ë¹„êµ
comparison = evaluator.compare_reports([
    "reports/rag_eval_baseline_v1.json",
    "reports/rag_eval_improved_v2.json"
])
```

## ğŸ“Š ì§€ì› ì§€í‘œ

| ì§€í‘œ            | ì„¤ëª…                             | ë²”ìœ„      |
| --------------- | -------------------------------- | --------- |
| **Recall@K**    | ì •ë‹µì´ top-kì— í¬í•¨ëœ ë¹„ìœ¨       | 0.0 - 1.0 |
| **Precision@K** | top-k ì¤‘ ì •ë‹µì¸ ë¹„ìœ¨             | 0.0 - 1.0 |
| **Hit@K**       | ì •ë‹µì´ top-kì— í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ 1 | 0 or 1    |
| **MRR**         | ì²« ë²ˆì§¸ ì •ë‹µì˜ ì—­ìˆœìœ„            | 0.0 - 1.0 |
| **NDCG@K**      | ìˆœìœ„ ê°€ì¤‘ ì •í™•ë„                 | 0.0 - 1.0 |

## ğŸ“ ë°ì´í„°ì…‹

### ì§€ì› í˜•ì‹

1. **JSON íŒŒì¼**

   ```json
   [{ "query": "ì§ˆë¬¸", "relevant_passages": ["ì •ë‹µ1", "ì •ë‹µ2"] }]
   ```

2. **HuggingFace Datasets**
   - Natural Questions
   - HotpotQA

### ìƒ˜í”Œ ë°ì´í„°ì…‹

`datasets/sample_qa.json` - 10ê°œì˜ ì‚¬ë‚´ FAQ QA ìŒ

## ğŸ“ˆ ë¦¬í¬íŠ¸ ì˜ˆì‹œ

```
============================================================
RAG Evaluation Report
============================================================
Dataset: baseline_v1
Samples: 100
Timestamp: 2026-01-13T12:00:00
------------------------------------------------------------
Metrics:
  hit@1: 0.4500
  hit@3: 0.6800
  hit@5: 0.7500
  mrr: 0.5234
  ndcg@5: 0.6123
  precision@5: 0.3200
  recall@5: 0.7500
============================================================
```

## ğŸ”„ ê°œì„  ì¶”ì 

1. ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì • â†’ `baseline_v1.json`
2. ê°œì„  ì ìš© (ì˜ˆ: Query Rewriting)
3. ì¬ì¸¡ì • â†’ `improved_v2.json`
4. ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±

## âš ï¸ ì£¼ì˜ì‚¬í•­

- ì‹¤ì œ DB ì—°ê²° í…ŒìŠ¤íŠ¸ëŠ” `@pytest.mark.skip` ì²˜ë¦¬ë˜ì–´ ìˆìŒ
- HuggingFace ë°ì´í„°ì…‹ ì‚¬ìš© ì‹œ `pip install datasets` í•„ìš”
