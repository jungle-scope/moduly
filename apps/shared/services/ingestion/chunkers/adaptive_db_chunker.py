"""
AdaptiveDbChunker: ê¸¸ì´ ê¸°ë°˜ ì¡°ê±´ë¶€ ì²­í‚¹

DB ë°ì´í„°ì˜ Jinja2 ë Œë”ë§ ê²°ê³¼ë¥¼ ê¸¸ì´ì— ë”°ë¼ ì¡°ê±´ë¶€ë¡œ ì²­í‚¹í•©ë‹ˆë‹¤.
- 1,000ì ì´í•˜: ë‹¨ì¼ ì²­í¬ ìœ ì§€
- 1,000ì ì´ˆê³¼: RecursiveSplitterë¡œ ë¶„í•  (150ì Overlap)
"""

from typing import Any, Dict, List, Optional

import tiktoken
from langchain_text_splitters import RecursiveCharacterTextSplitter


class AdaptiveDbChunker:
    """
    DB ë°ì´í„°ì˜ Jinja2 ë Œë”ë§ ê²°ê³¼ë¥¼ ê¸¸ì´ ê¸°ë°˜ìœ¼ë¡œ ì¡°ê±´ë¶€ ì²­í‚¹

    - 1,000ì ì´í•˜: ë‹¨ì¼ ì²­í¬ ìœ ì§€
    - 1,000ì ì´ˆê³¼: RecursiveSplitterë¡œ ë¶„í•  + Overlapìœ¼ë¡œ ë¬¸ë§¥ ë³´ì¡´
    """

    # ì²­í‚¹ ì„ê³„ê°’
    CHAR_THRESHOLD = 1000
    TOKEN_THRESHOLD = 800

    # ì•ˆì „ ì œí•œ (ì„ë² ë”© ëª¨ë¸ í•œê³„ì˜ ~60%)
    MAX_CHAR_LIMIT = 6000
    MAX_TOKEN_LIMIT = 5000

    # ì²­í‚¹ ì„¤ì • (Golden Ratio)
    CHUNK_SIZE = 1000
    OVERLAP = 150  # 15%

    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 150):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ".", " ", ""],
            keep_separator=True,
        )
        try:
            self.tokenizer = tiktoken.encoding_for_model("gpt-3.5-turbo")
        except Exception:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")

    def chunk_if_needed(
        self,
        text: str,
        metadata: Optional[Dict[str, Any]] = None,
        enable_chunking: bool = True,
    ) -> List[Dict[str, Any]]:
        """
        ì¡°ê±´ë¶€ ì²­í‚¹ ì‹¤í–‰ (ë¹„ìƒ ì²­í‚¹ í¬í•¨)

        Args:
            text: Jinja2 ë Œë”ë§ ê²°ê³¼ (ì´ë¯¸ ìì—°ì–´ ë¬¸ì¥)
            metadata: ì›ë³¸ ë©”íƒ€ë°ì´í„°
            enable_chunking: ìë™ ì²­í‚¹ í™œì„±í™” ì—¬ë¶€ (UI ì„¤ì •)

        Returns:
            List[{"content": str, "metadata": dict}]
        """
        import logging

        logger = logging.getLogger(__name__)

        # 1. ê¸¸ì´ ì¸¡ì •
        char_count = len(text)
        token_count = len(self.tokenizer.encode(text))

        # 2. ë¹„ìƒ ì²­í‚¹ ì„ê³„ê°’ (ì„ë² ë”© ëª¨ë¸ í•œê³„)
        EMERGENCY_THRESHOLD = 8000  # text-embedding-3-small í•œê³„ì˜ ~98%

        # 3. ë¹„ìƒ ì²­í‚¹ ê²€ì¦ (ì‚¬ìš©ì ì„¤ì • ë¬´ì‹œ)
        if token_count > EMERGENCY_THRESHOLD:
            logger.warning(
                f"ğŸš¨ Emergency Chunking í™œì„±í™”: {token_count} tokens > {EMERGENCY_THRESHOLD} limit. "
                f"ì„ë² ë”© ëª¨ë¸ í•œê³„ë¡œ ì¸í•´ ê°•ì œ ë¶„í• í•©ë‹ˆë‹¤."
            )

            # ë¹„ìƒ ì²­í‚¹ ì„¤ì • (í° ë©ì–´ë¦¬ ìœ ì§€)
            emergency_splitter = RecursiveCharacterTextSplitter(
                chunk_size=4000,  # 4,000 í† í° (ì•½ 16,000 chars)
                chunk_overlap=400,  # 400 í† í° (10%)
                length_function=lambda txt: len(self.tokenizer.encode(txt)),
                separators=["\n\n", "\n", ".", " ", ""],
                keep_separator=True,
            )

            raw_chunks = emergency_splitter.split_text(text)

            result_chunks = []
            for idx, chunk_content in enumerate(raw_chunks):
                chunk_tokens = len(self.tokenizer.encode(chunk_content))

                result_chunks.append(
                    {
                        "content": chunk_content,
                        "metadata": {
                            **(metadata or {}),
                            "char_count": len(chunk_content),
                            "token_count": chunk_tokens,
                            "chunked": True,
                            "emergency_chunked": True,
                            "chunk_index": idx,
                            "total_chunks": len(raw_chunks),
                        },
                    }
                )

            logger.info(f"âœ… Emergency Chunking ì™„ë£Œ: {len(raw_chunks)} chunks ìƒì„±")
            return result_chunks

        # 4. ì¼ë°˜ ì²­í‚¹ í•„ìš” ì—¬ë¶€ íŒë‹¨
        should_chunk = enable_chunking and (
            char_count > self.CHAR_THRESHOLD or token_count > self.TOKEN_THRESHOLD
        )

        if not should_chunk:
            # ë‹¨ì¼ ì²­í¬ ë°˜í™˜
            return [
                {
                    "content": text,
                    "metadata": {
                        **(metadata or {}),
                        "char_count": char_count,
                        "token_count": token_count,
                        "chunked": False,
                    },
                }
            ]

        # 5. RecursiveSplitterë¡œ ë¶„í•  (Overlapìœ¼ë¡œ ë¬¸ë§¥ ë³´ì¡´)
        raw_chunks = self.text_splitter.split_text(text)

        # 6. ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ë°˜í™˜
        result_chunks = []
        for idx, chunk_content in enumerate(raw_chunks):
            chunk_tokens = len(self.tokenizer.encode(chunk_content))

            result_chunks.append(
                {
                    "content": chunk_content,
                    "metadata": {
                        **(metadata or {}),
                        "char_count": len(chunk_content),
                        "token_count": chunk_tokens,
                        "chunked": True,
                        "chunk_index": idx,
                        "total_chunks": len(raw_chunks),
                    },
                }
            )

        return result_chunks
