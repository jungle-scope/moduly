import asyncio
import uuid
from typing import Any, Dict, List, Optional, Union

from sqlalchemy.orm import Session

from apps.shared.pubsub import (
    publish_workflow_event_async,  # [NEW] Async Redis Pub/Sub
)
from apps.shared.schemas.workflow import EdgeSchema, NodeSchema
from apps.workflow_engine.workflow.core.workflow_logger import (
    WorkflowLogger,
)
from apps.workflow_engine.workflow.core.workflow_node_factory import NodeFactory


class WorkflowEngine:
    """ë…¸ë“œì™€ ì—£ì§€ë¥¼ ë°›ì•„ì„œ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ì„ ë‹´ë‹¹í•˜ëŠ” ì—”ì§„ (AsyncIO ê¸°ë°˜)"""

    def __init__(
        self,
        graph: Union[Dict[str, Any], tuple[List[NodeSchema], List[EdgeSchema]]],
        user_input: Dict[str, Any] = None,
        execution_context: Dict[str, Any] = None,
        is_deployed: bool = False,
        db: Optional[Session] = None,
        parent_run_id: Optional[str] = None,
        workflow_timeout: int = 600,
        is_subworkflow: bool = False,
        entry_point_ids: Optional[List[str]] = None,
    ):
        """
        WorkflowEngine ì´ˆê¸°í™”
        """
        # graphê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° nodesì™€ edges ì¶”ì¶œ
        if isinstance(graph, dict):
            nodes = [NodeSchema(**node) for node in graph.get("nodes", [])]
            edges = [EdgeSchema(**edge) for edge in graph.get("edges", [])]
        else:
            nodes, edges = graph

        self.is_deployed = is_deployed
        self.node_schemas = {node.id: node for node in nodes}
        self.node_instances = {}
        self.edges = edges
        self.user_input = user_input if user_input is not None else {}
        # [FIX] execution_contextë¥¼ ìƒˆ ë³µì‚¬ë³¸ìœ¼ë¡œ ìƒì„±í•˜ì—¬ ì¤‘ì²© ì„œë¸Œ ì›Œí¬í”Œë¡œìš°ì—ì„œ ì°¸ì¡° ë¬¸ì œ ë°©ì§€
        self.execution_context = dict(execution_context) if execution_context else {}
        self.workflow_timeout = workflow_timeout  # [NEW] ì „ì²´ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        self.start_time = 0.0  # [NEW] ì‹¤í–‰ ì‹œì‘ ì‹œê°„

        # [FIX] DB ì„¸ì…˜ì„ execution_contextì— ì£¼ì… (WorkflowNode ë“±ì—ì„œ ì‚¬ìš©)
        # db íŒŒë¼ë¯¸í„°ê°€ ì „ë‹¬ë˜ë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ ê¸°ì¡´ execution_contextì˜ db ìœ ì§€
        if db is not None:
            self.execution_context["db"] = db
        elif "db" not in self.execution_context:
            # execution_contextì— dbê°€ ì—†ìœ¼ë©´ ê²½ê³  (ì˜µì…˜)
            pass

        self.adjacency_list = {}
        self.reverse_graph = {}
        self.edge_handles = {}
        self._build_optimized_graph()

        self.nodes_by_type = {}
        for node_id, schema in self.node_schemas.items():
            if schema.type not in self.nodes_by_type:
                self.nodes_by_type[schema.type] = []
            self.nodes_by_type[schema.type].append(node_id)

        self._build_node_instances()

        self.logger = WorkflowLogger(db)
        self.parent_run_id = parent_run_id
        self.start_node_id = None
        self.is_subworkflow = is_subworkflow

      
        self.entry_point_ids = entry_point_ids  # None ë˜ëŠ” ë¦¬ìŠ¤íŠ¸

        # [VALIDATION] ê·¸ë˜í”„ êµ¬ì¡° ê²€ì¦ (ìˆœí™˜, ì‹œì‘ ë…¸ë“œ ë“±)
        self.validate_graph()

    def cleanup(self):
        """
        ì‹¤í–‰ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬

        [FIX] ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•´ ëª¨ë“  ì°¸ì¡°ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.
        Celery íƒœìŠ¤í¬ì—ì„œ finally ë¸”ë¡ì—ì„œ í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
        """
        # [FIX] ë…¸ë“œ ì¸ìŠ¤í„´ìŠ¤ ë‚´ë¶€ì˜ ì„œë¸Œê·¸ë˜í”„ ì—”ì§„ë„ ì •ë¦¬ (LoopNode ë“±)
        for node_instance in self.node_instances.values():
            if (
                hasattr(node_instance, "_subgraph_engine")
                and node_instance._subgraph_engine
            ):
                node_instance._subgraph_engine.cleanup()
                node_instance._subgraph_engine = None

        # ë…¸ë“œ ê´€ë ¨ ì •ë¦¬
        self.node_instances.clear()
        self.node_schemas.clear()

        # ê·¸ë˜í”„ êµ¬ì¡° ì •ë¦¬
        self.adjacency_list.clear()
        self.reverse_graph.clear()
        self.edge_handles.clear()
        self.nodes_by_type.clear()

        # ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬
        self.execution_context.clear()
        self.user_input = None

        # ë¡œê±° ì •ë¦¬
        self.logger = None

        # ì—£ì§€ ì •ë¦¬
        self.edges = None

    async def execute(self) -> Dict[str, Any]:
        """
        ì›Œí¬í”Œë¡œìš° ì „ì²´ ì‹¤í–‰ (Wrapper)
        """
        if self.is_deployed:
            return await self.execute_deployed()

        final_context = {}
        async for event in self.execute_stream():
            if event["type"] == "workflow_finish":
                final_context = event["data"]
            elif event["type"] == "error":
                raise ValueError(event["data"]["message"])
        return final_context

    # ... execute_stream ...
    # (ì½”ë“œ ì¤‘ëµ ì—†ì´ í•„ìš”í•œ ë¶€ë¶„ë§Œ ìˆ˜ì •í•˜ê¸° ì–´ë ¤ìš°ë¯€ë¡œ __init__ë§Œ ìˆ˜ì •í•˜ê³  validate_graph ë“±ì€ ë³„ë„ ì²­í¬ë¡œ ìˆ˜ì •)
    # í•˜ì§€ë§Œ multi_replace_file_contentê°€ ì•„ë‹ˆë¯€ë¡œ í•œ ë²ˆì— í•  ìˆ˜ ìˆëŠ” ë§Œí¼ë§Œ.
    # __init__ì€ ìœ„ì—ì„œ ìˆ˜ì •ë¨.

    # ...

    async def execute(self) -> Dict[str, Any]:
        """
        ì›Œí¬í”Œë¡œìš° ì „ì²´ ì‹¤í–‰ (Wrapper)
        execute_streamì„ í˜¸ì¶œí•˜ì—¬ ì‹¤í–‰í•˜ê³ , ìµœì¢… ê²°ê³¼ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        if self.is_deployed:
            return await self.execute_deployed()

        final_context = {}
        async for event in self.execute_stream():
            if event["type"] == "workflow_finish":
                final_context = event["data"]
            elif event["type"] == "error":
                raise ValueError(event["data"]["message"])

        return final_context

    async def execute_stream(self):
        """
        ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ê³  ì§„í–‰ ìƒí™©ì„ ì œë„ˆë ˆì´í„°ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. (SSE ìŠ¤íŠ¸ë¦¬ë°ìš©)
        ê° ì‹¤í–‰ ë‹¨ê³„ë§ˆë‹¤ ì´ë²¤íŠ¸ë¥¼ yieldí•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒíƒœë¥¼ ì•Œ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

        Yields Events:
        - node_start: ë…¸ë“œ ì‹¤í–‰ ì‹œì‘
        - node_finish: ë…¸ë“œ ì‹¤í–‰ ì™„ë£Œ (ê²°ê³¼ í¬í•¨)
        - workflow_finish: ì „ì²´ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ
        - error: ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ
        """
        async for event in self._execute_core(stream_mode=True):
            yield event

    async def execute_deployed(self):
        """
        ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ë¡œì§
        streamingì´ í•„ìš” ì—†ëŠ” ë°°í¬ëœ workflowë¥¼ ì‹¤í–‰í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        # _execute_coreëŠ” ì œë„ˆë ˆì´í„°ì´ë¯€ë¡œ, ê°’ì„ ë°˜í™˜ë°›ìœ¼ë ¤ë©´ StopAsyncIterationì˜ valueë¥¼ ê°€ì ¸ì™€ì•¼ í•  ìˆ˜ë„ ìˆì§€ë§Œ
        # python async generatorëŠ” return ê°’ì„ ê°€ì§ˆ ìˆ˜ ì—†ìŒ (Python 3.6+).
        # ëŒ€ì‹  _execute_core ì•ˆì—ì„œ return í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë§ˆì§€ë§‰ì— ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë„ë¡ êµ¬ì¡°ë¥¼ ì¡ì•„ì•¼ í•¨.
        # í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” async for ë£¨í”„ë¥¼ ëŒë©´ì„œ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„.

        # NOTE: async generatorëŠ” return ê°’ì„ ê°€ì§ˆ ìˆ˜ ì—†ìŒ. ë”°ë¼ì„œ ì˜ˆì™¸ ì²˜ë¦¬ë¡œ ê°’ì„ ì „ë‹¬í•˜ê±°ë‚˜,
        # _execute_coreê°€ ë§ˆì§€ë§‰ì— íŠ¹ì • ì´ë²¤íŠ¸ë¥¼ yieldí•˜ê³  ì¢…ë£Œí•˜ë„ë¡ í•´ì•¼ í•¨.
        # ì—¬ê¸°ì„œëŠ” _execute_coreê°€ stream_mode=Falseì¼ ë•Œë„ workflow_finish ì´ë²¤íŠ¸ë¥¼ ì£¼ë„ë¡ í•˜ê±°ë‚˜
        # ë³„ë„ ë¡œì§ì„ ë¶„ë¦¬í•´ì•¼ í•¨.

        # ë¦¬íŒ©í† ë§: _execute_coreëŠ” í•­ìƒ ì´ë²¤íŠ¸ë¥¼ yieldí•˜ë„ë¡ í•˜ê³ , ì—¬ê¸°ì„œ ì²˜ë¦¬.
        final_result = None
        try:
            async for event in self._execute_core(stream_mode=False):
                if event["type"] == "workflow_finish":
                    final_result = event["data"]
                # ë°°í¬ ëª¨ë“œì—ì„œëŠ” ì¤‘ê°„ ì´ë²¤íŠ¸ ë¬´ì‹œ (ì—ëŸ¬ ì œì™¸)
        except Exception as e:
            raise e

        return final_result

    async def _execute_core(self, stream_mode: bool = False):
        """
        í•µì‹¬ ì‹¤í–‰ ë¡œì§ - ìŠ¤íŠ¸ë¦¬ë°/ë°°í¬ ëª¨ë“œ ê³µìš©
        [ì„±ëŠ¥ ê°œì„ ] AsyncIOë¥¼ ì´ìš©í•œ ë¹„ë™ê¸°/ë³‘ë ¬ ì‹¤í–‰
        [ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°] asyncio.Queueë¥¼ ì‚¬ìš©í•˜ì—¬ node_start/node_finish ì´ë²¤íŠ¸ ì¦‰ì‹œ ì „ë‹¬
        [ì¼ì •] íƒ€ì„ì•„ì›ƒ ì²´í¬ ë¡œì§ ì¶”ê°€
        """
        import time  # íƒ€ì„ì•„ì›ƒ ì²´í¬ìš©

        self.start_time = time.time()  # ì‹¤í–‰ ì‹œì‘ ì‹œê°„ ê¸°ë¡
        # ============================================================
      
        # ============================================================
        # ì™¸ë¶€ì—ì„œ ì „ë‹¬ëœ run_idê°€ ìˆìœ¼ë©´ ì‚¬ìš© (Gatewayì—ì„œ ë¯¸ë¦¬ ìƒì„±í•œ ê²½ìš°)
        external_run_id = self.execution_context.get("workflow_run_id")

        # [FIX] ì„œë¸Œ ì›Œí¬í”Œë¡œìš°ì¸ ê²½ìš° run_id ìƒì„±/ë¡œê¹… ìŠ¤í‚µ
        if self.is_subworkflow:
            # ì„œë¸Œ ì›Œí¬í”Œë¡œìš°ëŠ” ë¶€ëª¨ì˜ run_idë§Œ ì°¸ì¡°, DBì— ë³„ë„ run ë ˆì½”ë“œ ìƒì„±í•˜ì§€ ì•ŠìŒ
            if self.parent_run_id:
                self.logger.workflow_run_id = uuid.UUID(self.parent_run_id)
                self.execution_context["workflow_run_id"] = self.parent_run_id
        elif external_run_id:
            # ì™¸ë¶€ run_id ì‚¬ìš© (Gateway â†’ Celery â†’ WorkflowEngine)
            self.logger.workflow_run_id = uuid.UUID(external_run_id)
            # [PERF] ë¡œê¹… ë¹„ë™ê¸° ì‹¤í–‰ (ìŠ¤ë ˆë“œ í’€)
            loop = asyncio.get_running_loop()
            await loop.run_in_executor(
                None,
                lambda: self.logger.create_run_log(
                    workflow_id=self.execution_context.get("workflow_id"),
                    user_id=self.execution_context.get("user_id"),
                    user_input=self.user_input,
                    is_deployed=self.is_deployed,
                    execution_context=self.execution_context,
                    external_run_id=external_run_id,  # [NEW] ì™¸ë¶€ run_id ì „ë‹¬
                ),
            )
        elif self.parent_run_id:
            # ì„œë¸Œ ì›Œí¬í”Œë¡œìš°ì¸ ê²½ìš° ë¶€ëª¨ì˜ run_idë¥¼ ì¬ì‚¬ìš© (ë ˆê±°ì‹œ ì§€ì›)
            self.logger.workflow_run_id = uuid.UUID(self.parent_run_id)
            self.execution_context["workflow_run_id"] = self.parent_run_id
        else:
            # ìƒˆë¡œìš´ run_id ìƒì„± (ë¹„ë™ê¸° ì²˜ë¦¬ ë¶ˆê°€ - run_idê°€ í•„ìš”í•¨)
            # í•˜ì§€ë§Œ create_run_log ë‚´ë¶€ì˜ Celery í˜¸ì¶œë§Œ ë¹„ë™ê¸°í™”í•˜ê³ , UUID ìƒì„±ì€ ë™ê¸° ì²˜ë¦¬ ê°€ëŠ¥
            # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ ë™ê¸°ë¡œ ì²˜ë¦¬í•˜ë˜, Celery í˜¸ì¶œì´ ë¸”ë¡œí‚¹ë˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•´ì•¼ í•¨.
            # WorkflowLoggerëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ë™ê¸° Celery í˜¸ì¶œì„ í•˜ë¯€ë¡œ,
            # run_idë¥¼ ë¨¼ì € ìƒì„±í•˜ê³  ë¡œê¹…ì€ ë‚˜ì¤‘ì— í•˜ê±°ë‚˜, run_in_executorì—ì„œ ë°˜í™˜ê°’ì„ ë°›ì•„ì•¼ í•¨.

            # [FIX] run_id ìƒì„±ì„ ìœ„í•´ run_in_executor ì‚¬ìš© ë° ê²°ê³¼ ëŒ€ê¸°
            loop = asyncio.get_running_loop()

            def _create_log():
                return self.logger.create_run_log(
                    workflow_id=self.execution_context.get("workflow_id"),
                    user_id=self.execution_context.get("user_id"),
                    user_input=self.user_input,
                    is_deployed=self.is_deployed,
                    execution_context=self.execution_context,
                )

            workflow_run_id = await loop.run_in_executor(None, _create_log)

            if workflow_run_id:
                self.execution_context["workflow_run_id"] = str(workflow_run_id)
        # ============================================================

      
        if self.entry_point_ids is not None and len(self.entry_point_ids) > 0:
            initial_nodes = self.entry_point_ids
        else:
            initial_nodes = [self._find_start_node()]

        results = {}

        # ë³‘ë ¬ ì‹¤í–‰ ìƒíƒœ ê´€ë¦¬
        executed_nodes = set()
        queued_nodes = set(initial_nodes)

        # AsyncIO Task ê´€ë¦¬
        # running_tasks: {Task: node_id}
        running_tasks = {}

        # Max Workers (AsyncIO Semaphoreë¡œ ì œì–´)
        max_concurrent_tasks = 10
        semaphore = asyncio.Semaphore(max_concurrent_tasks)

        # [ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°] ì´ë²¤íŠ¸ í ìƒì„± - node_start/node_finish ì´ë²¤íŠ¸ë¥¼ ì¦‰ì‹œ ì „ë‹¬í•˜ê¸° ìœ„í•¨
        event_queue: asyncio.Queue = asyncio.Queue() if stream_mode else None

        try:
            # 1. ì›Œí¬í”Œë¡œìš° ì‹œì‘ ì´ë²¤íŠ¸ (ìŠ¤íŠ¸ë¦¼ ëª¨ë“œë§Œ)
            if stream_mode:
                yield {"type": "workflow_start", "data": {}}

            # ì´ˆê¸° ì‹œì‘ ë…¸ë“œ(ë“¤) ì‹¤í–‰ íƒœìŠ¤í¬ ìƒì„±
            for node_id in initial_nodes:
                await self._submit_node(
                    node_id,
                    results,
                    running_tasks,
                    stream_mode,
                    semaphore,
                    event_queue,
                )

            while running_tasks:
              
                elapsed_time = time.time() - self.start_time
                if elapsed_time > self.workflow_timeout:
                    # ëª¨ë“  ì‹¤í–‰ ì¤‘ì¸ íƒœìŠ¤í¬ ì·¨ì†Œ
                    for t in running_tasks:
                        t.cancel()

                    error_msg = (
                        f"Workflow timed out after {self.workflow_timeout} seconds."
                    )
                    raise TimeoutError(error_msg)

                # [ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°] ì´ë²¤íŠ¸ íì—ì„œ ëŒ€ê¸° ì¤‘ì¸ ì´ë²¤íŠ¸ë¥¼ ë¨¼ì € ëª¨ë‘ ì „ë‹¬
                if stream_mode and event_queue:
                    while not event_queue.empty():
                        event = event_queue.get_nowait()
                        yield event

                # ì™„ë£Œëœ ì‘ì—… ëŒ€ê¸° (ì§§ì€ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì´ë²¤íŠ¸ íë„ ì£¼ê¸°ì ìœ¼ë¡œ í™•ì¸)
                # asyncio.waitëŠ” (done, pending) íŠœí”Œ ë°˜í™˜
                try:
                    done, _ = await asyncio.wait(
                        running_tasks.keys(),
                        timeout=0.05
                        if stream_mode
                        else None,  # ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ: 50ms íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì´ë²¤íŠ¸ í í™•ì¸
                        return_when=asyncio.FIRST_COMPLETED,
                    )
                except asyncio.CancelledError:
                    raise

                # íƒ€ì„ì•„ì›ƒìœ¼ë¡œ doneì´ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ (ìŠ¤íŠ¸ë¦¼ ëª¨ë“œì—ì„œ ì´ë²¤íŠ¸ ì²˜ë¦¬ìš©)
                if not done:
                    continue

                for task in done:
                    node_id = running_tasks.pop(task)
                    executed_nodes.add(node_id)

                    try:
                        # ì‹¤í–‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ì˜ˆì™¸ ë°œìƒ ì‹œ ì—¬ê¸°ì„œ raiseë¨)
                        result_data = task.result()
                        node_result = result_data["result"]

                        # ê²°ê³¼ ì €ì¥
                        results[node_id] = node_result

                    except Exception as e:
                        # ì—ëŸ¬ ì²˜ë¦¬
                        error_msg = str(e)
                        self.logger.update_run_log_error(error_msg)

                        if stream_mode:
                            yield {
                                "type": "error",
                                "data": {"node_id": node_id, "message": error_msg},
                            }

                        # ì‹¤í–‰ ì¤‘ì¸ ëª¨ë“  íƒœìŠ¤í¬ ì·¨ì†Œ
                        for t in running_tasks:
                            t.cancel()

                        raise e  # ì¦‰ì‹œ ì¤‘ë‹¨

                    # ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ íƒìƒ‰ ë° ì œì¶œ
                    next_nodes = self._get_next_nodes(node_id, results[node_id])
                    for next_node_id in next_nodes:
                        # ì•„ì§ ì‹¤í–‰ ì•ˆëê³ , íì— ì—†ê³ , í˜„ì¬ ì‹¤í–‰ ì¤‘ì´ì§€ ì•Šìœ¼ë©°, ëª¨ë“  ì„ í–‰ ë…¸ë“œê°€ ì™„ë£Œë˜ì—ˆìœ¼ë©´ ì‹¤í–‰
                        if (
                            next_node_id not in executed_nodes
                            and next_node_id not in queued_nodes
                            and next_node_id not in running_tasks.values()
                            and self._is_ready(next_node_id, results)
                        ):
                            queued_nodes.add(next_node_id)
                            await self._submit_node(
                                next_node_id,
                                results,
                                running_tasks,
                                stream_mode,
                                semaphore,
                                event_queue,
                            )

            # [ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°] ë‚¨ì€ ì´ë²¤íŠ¸ ëª¨ë‘ ì „ë‹¬
            if stream_mode and event_queue:
                while not event_queue.empty():
                    event = event_queue.get_nowait()
                    yield event

            # 4. ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ
            run_id = self.execution_context.get("workflow_run_id")
            if stream_mode:
                final_context = dict(results)
                if not self.is_subworkflow:
                    loop = asyncio.get_running_loop()
                    await loop.run_in_executor(
                        None, lambda: self.logger.update_run_log_finish(final_context)
                    )
                # [FIX] ì„œë¸Œ ì›Œí¬í”Œë¡œìš°ì—ì„œëŠ” Redis ì´ë²¤íŠ¸ ë°œí–‰ ìŠ¤í‚µ (ì¡°ê¸° ì¢…ë£Œ ë°©ì§€)
                if run_id and not self.is_subworkflow:
                    await publish_workflow_event_async(
                        run_id, "workflow_finish", final_context
                    )
                yield {"type": "workflow_finish", "data": final_context}
            else:
                final_result = self._get_answer_node_result(results)
                if not self.is_subworkflow:
                    loop = asyncio.get_running_loop()
                    await loop.run_in_executor(
                        None, lambda: self.logger.update_run_log_finish(final_result)
                    )
                # [FIX] ì„œë¸Œ ì›Œí¬í”Œë¡œìš°ì—ì„œëŠ” Redis ì´ë²¤íŠ¸ ë°œí–‰ ìŠ¤í‚µ
                if run_id and not self.is_subworkflow:
                    await publish_workflow_event_async(
                        run_id, "workflow_finish", final_result
                    )
                # ë°°í¬ ëª¨ë“œì—ì„œë„ ê²°ê³¼ ì „ë‹¬ì„ ìœ„í•´ ì´ë²¤íŠ¸ ì‚¬ìš©
                yield {"type": "workflow_finish", "data": final_result}

        except Exception as e:
            run_id = self.execution_context.get("workflow_run_id")
            if not stream_mode:
                if not self.is_subworkflow:
                    loop = asyncio.get_running_loop()
                    await loop.run_in_executor(
                        None, lambda: self.logger.update_run_log_error(str(e))
                    )
                # [FIX] ì„œë¸Œ ì›Œí¬í”Œë¡œìš°ì—ì„œëŠ” Redis ì´ë²¤íŠ¸ ë°œí–‰ ìŠ¤í‚µ
                if run_id and not self.is_subworkflow:
                    await publish_workflow_event_async(
                        run_id, "error", {"message": str(e)}
                    )
                raise e
            else:
                error_msg = str(e)
                if not self.is_subworkflow:
                    loop = asyncio.get_running_loop()
                    await loop.run_in_executor(
                        None, lambda: self.logger.update_run_log_error(error_msg)
                    )
                # [FIX] ì„œë¸Œ ì›Œí¬í”Œë¡œìš°ì—ì„œëŠ” Redis ì´ë²¤íŠ¸ ë°œí–‰ ìŠ¤í‚µ
                if run_id and not self.is_subworkflow:
                    await publish_workflow_event_async(
                        run_id, "error", {"message": error_msg}
                    )
                yield {"type": "error", "data": {"message": error_msg}}
        # ì°¸ê³ : self.logger.shutdown() í˜¸ì¶œ ì œê±°ë¨
        # ì´ì œ ê³µìœ  LogWorkerPoolì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì¸ìŠ¤í„´ìŠ¤ë³„ ì¢…ë£Œ ë¶ˆí•„ìš”
        # í’€ì€ ì•± ì¢…ë£Œ ì‹œ shutdown_log_worker_pool()ìœ¼ë¡œ ì¢…ë£Œë¨

    async def _submit_node(
        self, node_id, results, running_tasks, stream_mode, semaphore, event_queue
    ):
        """
        ê°œë³„ ë…¸ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•´ AsyncIO Task ìƒì„±
        [ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°] node_start ì´ë²¤íŠ¸ë¥¼ ì¦‰ì‹œ ì „ì†¡
        """
        # node_id ê²€ì¦
        if node_id not in self.node_instances:
            raise ValueError(f"ë…¸ë“œ ID '{node_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        node_instance = self.node_instances[node_id]
        node_schema = self.node_schemas[node_id]

        # ì»¨í…ìŠ¤íŠ¸ ë³µì‚¬ (ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´ì¥ í•„ìš” ì‹œ)
        inputs = self._get_context(node_id, results)

        # [ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°] node_start ì´ë²¤íŠ¸ë¥¼ Task ìƒì„± ì‹œì (ì‹¤í–‰ ì‹œì‘ ì „)ì— ì¦‰ì‹œ ì „ì†¡
        # [FIX] ì„œë¸Œ ì›Œí¬í”Œë¡œìš°ì—ì„œëŠ” ë…¸ë“œ ë¡œê¹…ë„ ìŠ¤í‚µ (UI ê°„ì„­ ë°©ì§€)
        # [NEW] Upsert íŒ¨í„´ì„ ìœ„í•´ started_at ê¸°ë¡ (Race Condition í•´ê²°ìš©)
        from datetime import datetime, timezone
        started_at = datetime.now(timezone.utc)

        # [NEW] ë…¸ë“œ ì˜µì…˜ ìŠ¤ëƒ…ìƒ· ì¶”ì¶œ (ì„œë¸Œì›Œí¬í”Œë¡œìš° ì•„ë‹ ë•Œë§Œ)
        node_options_snapshot = None
        log_id = None

        if not self.is_subworkflow:
            node_options_snapshot = self._extract_node_options(node_schema)
            
            # [FIX] create_node_logê°€ ë°˜í™˜í•˜ëŠ” log_id ìº¡ì²˜
            def _create_log():
                return self.logger.create_node_log(
                    node_id,
                    node_schema.type,
                    inputs,
                    process_data=node_options_snapshot,
                )

            loop = asyncio.get_running_loop()
            log_id = await loop.run_in_executor(None, _create_log)

        # [FIX] Redis Pub/Subìœ¼ë¡œ ì´ë²¤íŠ¸ ë°œí–‰ (run_idê°€ ìˆê³  ì„œë¸Œì›Œí¬í”Œë¡œìš°ê°€ ì•„ë‹ ê²½ìš°)
        # [PERF] ë¹„ë™ê¸° ë°œí–‰ ì‚¬ìš©
        run_id = self.execution_context.get("workflow_run_id")
        if run_id and not self.is_subworkflow:
            await publish_workflow_event_async(
                run_id,
                "node_start",
                {
                    "node_id": node_id,
                    "node_type": node_schema.type,
                },
            )

        if stream_mode and event_queue:
            await event_queue.put(
                {
                    "type": "node_start",
                    "data": {"node_id": node_id, "node_type": node_schema.type},
                }
            )

        async def _task_wrapper():
            async with semaphore:
                # ë¹„ë™ê¸° ë…¸ë“œ ì‹¤í–‰ + Upsertìš© ì¶”ê°€ ì •ë³´ ì „ë‹¬
                return await self._execute_node_task_async(
                    node_id,
                    node_schema,
                    node_instance,
                    inputs,
                    log_id,
                    node_options_snapshot,  # [NEW] Upsertìš©
                    started_at,  # [NEW] Upsertìš©
                )

      
        # ìš°ì„ ìˆœìœ„: 1. ë…¸ë“œ ì„¤ì •(node_schema.timeout) > 2. ê¸°ë³¸ê°’(300ì´ˆ)
        node_timeout = node_schema.timeout if node_schema.timeout is not None else 300

        # [ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°] node_finish ì´ë²¤íŠ¸ë¥¼ ì™„ë£Œ ì‹œì ì— ì¦‰ì‹œ ì „ì†¡í•˜ëŠ” ë˜í¼
        async def _task_wrapper_with_event():
            try:
                # [TIMEOUT] ë…¸ë“œ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ ì ìš©
                result = await asyncio.wait_for(_task_wrapper(), timeout=node_timeout)
            except asyncio.TimeoutError:
                raise TimeoutError(
                    f"Node '{node_id}' ({node_schema.type}) timed out after {node_timeout} seconds."
                )

            # [FIX] Redis Pub/Subìœ¼ë¡œ node_finish ì´ë²¤íŠ¸ ë°œí–‰ (run_idê°€ ìˆê³  ì„œë¸Œì›Œí¬í”Œë¡œìš°ê°€ ì•„ë‹ ê²½ìš°)
            # [PERF] ë¹„ë™ê¸° ë°œí–‰ ì‚¬ìš©
            run_id = self.execution_context.get("workflow_run_id")
            if run_id and not self.is_subworkflow:
                await publish_workflow_event_async(
                    run_id,
                    "node_finish",
                    {
                        "node_id": node_id,
                        "node_type": node_schema.type,
                        "output": result,
                    },
                )

            # node_finish ì´ë²¤íŠ¸ë¥¼ ì¦‰ì‹œ íì— ì „ì†¡
            if stream_mode and event_queue:
                await event_queue.put(
                    {
                        "type": "node_finish",
                        "data": {
                            "node_id": node_id,
                            "node_type": node_schema.type,
                            "output": result,
                        },
                    }
                )

            return {"result": result}

        # Task ìƒì„± ë° ë“±ë¡
        task = asyncio.create_task(_task_wrapper_with_event())
        running_tasks[task] = node_id

    async def _execute_node_task_async(
        self,
        node_id,
        node_schema,
        node_instance,
        inputs,
        log_id=None,
        node_options_snapshot=None,
        started_at=None,
    ):
        """
        ê°œë³„ ë…¸ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ì‘ì—… (ë¹„ë™ê¸° ì‹¤í–‰)
        [ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°] ì´ë²¤íŠ¸ëŠ” _submit_nodeì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ê²°ê³¼ë§Œ ë°˜í™˜
        [Upsert íŒ¨í„´] ì¶”ê°€ ì •ë³´ë¥¼ ì „ë‹¬í•˜ì—¬ Race Condition í•´ê²°
        ë°˜í™˜ê°’: ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼ (Dict)
        """
        try:
            # ë…¸ë“œ ì‹¤í–‰ (í•µì‹¬) - ë¹„ë™ê¸° ì‹¤í–‰
            result = await node_instance.execute(inputs)

            # ë…¸ë“œ ì™„ë£Œ ë¡œê¹… (ì„œë¸Œ ì›Œí¬í”Œë¡œìš°ì—ì„œëŠ” ìŠ¤í‚µ)
            # [FIX] Upsertìš© ì¶”ê°€ ì •ë³´ ì „ë‹¬
            if not self.is_subworkflow:
                loop = asyncio.get_running_loop()
                await loop.run_in_executor(
                    None,
                    lambda: self.logger.update_node_log_finish(
                        log_id,
                        node_id,
                        result,
                        node_type=node_schema.type,
                        inputs=inputs,
                        process_data=node_options_snapshot,
                        started_at=started_at,
                    ),
                )

            return result

        except Exception as e:
            error_msg = str(e)
            # [FIX] Upsertìš© ì¶”ê°€ ì •ë³´ ì „ë‹¬
            if not self.is_subworkflow:
                loop = asyncio.get_running_loop()
                await loop.run_in_executor(
                    None,
                    lambda: self.logger.update_node_log_error(
                        log_id,
                        node_id,
                        error_msg,
                        node_type=node_schema.type,
                        inputs=inputs,
                        process_data=node_options_snapshot,
                        started_at=started_at,
                    ),
                )
            raise e

    # ================================================================
  
    # ================================================================

    def validate_graph(self):
        """
        ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ì˜ êµ¬ì¡°ì  ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
        1. ìˆœí™˜(Cycle) ì—¬ë¶€ ê²€ì‚¬
        2. ì‹œì‘ ë…¸ë“œ ê°œìˆ˜ ê²€ì‚¬ (0ê°œ ë˜ëŠ” 2ê°œ ì´ìƒì´ë©´ ì—ëŸ¬)
        3. ë„ë‹¬ ë¶ˆê°€ëŠ¥í•œ ê³ ë¦½ ë…¸ë“œ ê²€ì‚¬
        """
        self._check_cycles()
        self._check_start_nodes()
        self._check_isolation()

    def _check_cycles(self):
        """DFSë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ ë‚´ ìˆœí™˜(Cycle)ì„ ê°ì§€í•©ë‹ˆë‹¤."""
        visited = set()
        recursion_stack = set()

        for node_id in self.node_schemas:
            if node_id not in visited:
                if self._detect_cycle_dfs(node_id, visited, recursion_stack):
                    raise ValueError(
                        f"ì›Œí¬í”Œë¡œìš°ì— ìˆœí™˜(Cycle)ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë…¸ë“œ ID: {node_id}"
                    )

    def _detect_cycle_dfs(self, node_id, visited, recursion_stack):
        """ìˆœí™˜ ê°ì§€ë¥¼ ìœ„í•œ DFS ì¬ê·€ í•¨ìˆ˜"""
        visited.add(node_id)
        recursion_stack.add(node_id)

        # ì¸ì ‘ ë…¸ë“œ íƒìƒ‰
        for neighbor in self.adjacency_list.get(node_id, []):
            if neighbor not in visited:
                if self._detect_cycle_dfs(neighbor, visited, recursion_stack):
                    return True
            elif neighbor in recursion_stack:
                return True

        recursion_stack.remove(node_id)
        return False

    def _check_start_nodes(self):
        """ì‹œì‘ ë…¸ë“œ ìœ íš¨ì„± ê²€ì‚¬ (0ê°œ ë˜ëŠ” 2ê°œ ì´ìƒ ë¶ˆê°€) ë° ID ìºì‹±"""

      
        if self.entry_point_ids is not None:
            # ì§„ì…ì ì´ ìœ íš¨í•œ ë…¸ë“œ IDì¸ì§€ í™•ì¸
            for pid in self.entry_point_ids:
                if pid not in self.node_schemas:
                    raise ValueError(f"ì§€ì •ëœ ì§„ì…ì  ë…¸ë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pid}")
            return

        start_nodes = []

        # Trigger ë…¸ë“œ íƒ€ì… ì •ì˜
        TRIGGER_TYPES = ["startNode", "webhookTrigger", "scheduleTrigger"]

        for node_id, node in self.node_schemas.items():
            if node.type in TRIGGER_TYPES:
                start_nodes.append(node_id)

        if len(start_nodes) > 1:
            raise ValueError(
                f"ì›Œí¬í”Œë¡œìš°ì— ì‹œì‘ ë…¸ë“œê°€ {len(start_nodes)}ê°œ ìˆìŠµë‹ˆë‹¤. ì‹œì‘ ë…¸ë“œëŠ” 1ê°œë§Œ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
            )
        elif len(start_nodes) == 0:
            raise ValueError(
                "ì›Œí¬í”Œë¡œìš°ì— ì‹œì‘ ë…¸ë“œ(type='startNode' or 'webhookTrigger')ê°€ ì—†ìŠµë‹ˆë‹¤."
            )

      
        self.start_node_id = start_nodes[0]

    def _check_isolation(self):
        """
        ì‹œì‘ ë…¸ë“œì—ì„œ ë„ë‹¬ ë¶ˆê°€ëŠ¥í•œ ê³ ë¦½(Isolated) ë…¸ë“œê°€ ìˆëŠ”ì§€ ê²€ì‚¬í•©ë‹ˆë‹¤.
        BFSë¥¼ ì‚¬ìš©í•˜ì—¬ ë„ë‹¬ ê°€ëŠ¥í•œ ëª¨ë“  ë…¸ë“œë¥¼ íƒìƒ‰í•˜ê³ , ì „ì²´ ë…¸ë“œì™€ ë¹„êµí•©ë‹ˆë‹¤.

        Loop Nodeì˜ ìì‹ ë…¸ë“œë“¤ì€ parentIdë¡œ ì—°ê²°ë˜ì–´ ìˆìœ¼ë¯€ë¡œ íŠ¹ë³„íˆ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
      
        if self.entry_point_ids is not None:
            return

        start_node_id = self._find_start_node()
        visited = {start_node_id}
        queue = [start_node_id]

        while queue:
            current_node = queue.pop(0)
            neighbors = self.adjacency_list.get(current_node, [])
            for neighbor in neighbors:
                if neighbor not in visited:
                    visited.add(neighbor)
                    queue.append(neighbor)

            # Loop Nodeì˜ ìì‹ ë…¸ë“œë“¤ë„ ë°©ë¬¸ ì²˜ë¦¬
            # parentIdê°€ current_nodeì¸ ë…¸ë“œë“¤ì„ ì°¾ì•„ì„œ visitedì— ì¶”ê°€
            for node_id, node_schema in self.node_schemas.items():
                # parentId í™•ì¸: node_schema ê°ì²´ì˜ ì†ì„±ì„ ìš°ì„  í™•ì¸
                parent_id = None

                # 1. node_schema ê°ì²´ì˜ ì§ì ‘ ì†ì„±ìœ¼ë¡œ í™•ì¸ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
                if hasattr(node_schema, "parentId"):
                    parent_id = node_schema.parentId
                elif hasattr(node_schema, "parent_id"):
                    parent_id = node_schema.parent_id
                # 2. data ë”•ì…”ë„ˆë¦¬ ì•ˆì—ì„œ í™•ì¸ (fallback)
                elif hasattr(node_schema, "data") and isinstance(
                    node_schema.data, dict
                ):
                    parent_id = node_schema.data.get(
                        "parentId"
                    ) or node_schema.data.get("parent_id")

                if parent_id == current_node and node_id not in visited:
                    visited.add(node_id)
                    queue.append(node_id)

        # ì „ì²´ ë…¸ë“œ ì§‘í•©
        all_nodes = set(self.node_schemas.keys())

        # ì£¼ì„/ë©”ëª¨ ë…¸ë“œ ì œì™¸ (ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ë³´í†µ ë©”ëª¨ëŠ” ì‹¤í–‰ íë¦„ê³¼ ë¬´ê´€)
        # ë§Œì•½ note íƒ€ì…ì´ node_schemasì— í¬í•¨ëœë‹¤ë©´ ì œì™¸í•´ì•¼ í•¨.
        # ì—¬ê¸°ì„œëŠ” self._build_node_instancesì—ì„œ note íƒ€ì…ì„ ê±´ë„ˆë›°ì§€ë§Œ,
        # node_schemasì—ëŠ” í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ.
        valid_nodes = {
            node_id
            for node_id in all_nodes
            if self.node_schemas[node_id].type != "note"
        }

        # ê³ ë¦½ëœ ë…¸ë“œ ì‹ë³„ (ë„ë‹¬ ë¶ˆê°€ëŠ¥í•œ ë…¸ë“œ)
        # ë‹¨, parentIdê°€ ìˆëŠ” ë…¸ë“œ(ì„œë¸Œê·¸ë˜í”„ ë…¸ë“œ)ëŠ” ì œì™¸
        isolated_nodes = set()
        for node_id in valid_nodes - visited:
            node_schema = self.node_schemas[node_id]

            # parentId í™•ì¸
            has_parent = False
            if hasattr(node_schema, "parentId") and node_schema.parentId:
                has_parent = True
            elif hasattr(node_schema, "parent_id") and node_schema.parent_id:
                has_parent = True
            elif hasattr(node_schema, "data") and isinstance(node_schema.data, dict):
                if node_schema.data.get("parentId") or node_schema.data.get(
                    "parent_id"
                ):
                    has_parent = True

            # parentIdê°€ ì—†ëŠ” ë…¸ë“œë§Œ ê³ ë¦½ ë…¸ë“œë¡œ íŒë‹¨
            if not has_parent:
                isolated_nodes.add(node_id)

        if isolated_nodes:
            raise ValueError(
                f"ì‹œì‘ ë…¸ë“œì—ì„œ ë„ë‹¬í•  ìˆ˜ ì—†ëŠ” ê³ ë¦½ëœ ë…¸ë“œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. "
                f"ë…¸ë“œ IDs: {list(isolated_nodes)}"
            )

    # ================================================================
    # ê¸°ì¡´ í—¬í¼ ë©”ì„œë“œë“¤ (ë³€ê²½ ì—†ìŒ)
    # ================================================================

    def _find_start_node(self) -> str:
        """
        ì‹œì‘ ë…¸ë“œ ì°¾ê¸°
        validate_graph()ì—ì„œ ì´ë¯¸ ê²€ì¦ë˜ê³  self.start_node_idì— ìºì‹±ë˜ì—ˆìœ¼ë¯€ë¡œ ë°”ë¡œ ë°˜í™˜
        """
        if self.start_node_id is None:
            # í˜¹ì‹œ ëª¨ë¥¼ ì˜ˆì™¸ ìƒí™© (validate_graphê°€ í˜¸ì¶œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¡œì§ ì˜¤ë¥˜)
            raise ValueError(
                "ì‹œì‘ ë…¸ë“œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. validate_graph()ë¥¼ ë¨¼ì € í˜¸ì¶œí•´ì£¼ì„¸ìš”."
            )

        return self.start_node_id

    def _get_next_nodes(self, node_id: str, result: Dict[str, Any]) -> List[str]:
        """
        í˜„ì¬ ë…¸ë“œì˜ ë‹¤ìŒ ë…¸ë“œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

        ë™ì‘ ë°©ì‹:
        1. selected_handle is None (ê¸°ë³¸ ë™ì‘):
           - "íŠ¹ì • ê²½ë¡œë¥¼ ì„ íƒí•˜ì§€ ì•ŠìŒ"ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
           - ì—°ê²°ëœ ëª¨ë“  ì—£ì§€ë¥¼ ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë“¤ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. (Parallel ì‹¤í–‰ ê°€ëŠ¥)
           - [PERF] ë¯¸ë¦¬ êµ¬ì¶•ëœ self.graphë¥¼ ì‚¬ìš©í•˜ì—¬ O(1) ì¡°íšŒ

        2. selected_handle has value (ë¶„ê¸° ë™ì‘):
           - "íŠ¹ì • í•¸ë“¤(ê²½ë¡œ)ë§Œ ì„ íƒí•¨"ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
           - ì—£ì§€ì˜ sourceHandleì´ selected_handleê³¼ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ì—ë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.
           - ì˜ˆ: IF ë…¸ë“œì—ì„œ ì¡°ê±´ì— ë”°ë¼ 'True' ë˜ëŠ” 'False' ê²½ë¡œ ì¤‘ í•˜ë‚˜ë§Œ ì‹¤í–‰.

        Args:
            node_id: í˜„ì¬ ë…¸ë“œ ID
            result: í˜„ì¬ ë…¸ë“œì˜ ì‹¤í–‰ ê²°ê³¼

        Returns:
            ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ ID ëª©ë¡
        """
        selected_handle = result.get("selected_handle")

<<<<<<< HEAD
        # ğŸ” DEBUG: Track sourceHandle matching for condition nodes (Deleted)

=======
>>>>>>> e83c0166696d2f237405f1282a7eff4d2a0005dc
        # [PERF] ë¶„ê¸°ê°€ ìˆëŠ” ê²½ìš° (O(1))
        if selected_handle is not None:
            key = (node_id, selected_handle)
            next_nodes = self.edge_handles.get(key, [])
<<<<<<< HEAD
            # ğŸ” DEBUG: Track matching result (Deleted)
=======

>>>>>>> e83c0166696d2f237405f1282a7eff4d2a0005dc
            return next_nodes

        # [PERF] ë¶„ê¸°ê°€ ì—†ëŠ” ê²½ìš° (O(1))
        return self.adjacency_list.get(node_id, [])

    def _is_ready(self, node_id: str, results: Dict) -> bool:
        """
        í˜„ì¬ ë…¸ë“œì— ì„ í–‰ë˜ëŠ” ë…¸ë“œê°€ ëª¨ë‘ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸

        [PERF] reverse_graph ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ O(1) ì¡°íšŒ (ê¸°ì¡´: O(E) ìˆœíšŒ)
        """
        required_inputs = self.reverse_graph.get(node_id, [])
        return all(inp in results for inp in required_inputs)

    def _build_optimized_graph(self):
        """ì—£ì§€ë¥¼ ë¶„ì„í•˜ì—¬ íš¨ìœ¨ì ì¸ ê·¸ë˜í”„ êµ¬ì¡° ìƒì„± (O(E) í•œ ë²ˆë§Œ)"""
        for edge in self.edges:
<<<<<<< HEAD
            # ğŸ” DEBUG: Track edge data with sourceHandle (Deleted)

=======
>>>>>>> e83c0166696d2f237405f1282a7eff4d2a0005dc
            # ì •ë°©í–¥ ê·¸ë˜í”„ (source -> targets)
            if edge.source not in self.adjacency_list:
                self.adjacency_list[edge.source] = []
            self.adjacency_list[edge.source].append(edge.target)

            # ì—­ë°©í–¥ ê·¸ë˜í”„ (target -> sources) - _is_ready ìµœì í™”ìš©
            if edge.target not in self.reverse_graph:
                self.reverse_graph[edge.target] = []
            self.reverse_graph[edge.target].append(edge.source)

            # í•¸ë“¤ë³„ ì—£ì§€ ë§¤í•‘ (ë¶„ê¸° ì²˜ë¦¬ ìµœì í™”)
            key = (edge.source, edge.sourceHandle)
            if key not in self.edge_handles:
                self.edge_handles[key] = []
            self.edge_handles[key].append(edge.target)

    def _build_node_instances(self):
        """NodeSchemaë¥¼ ì‹¤ì œ Node ì¸ìŠ¤í„´ìŠ¤ë¡œ ë³€í™˜ (NodeFactory ì‚¬ìš©)"""
        for node_id, schema in self.node_schemas.items():
            # ë©”ëª¨ ë…¸ë“œëŠ” UI ì „ìš©ì´ë¯€ë¡œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ìŠ¤í‚µ
            if schema.type == "note":
                continue

            try:
                self.node_instances[node_id] = NodeFactory.create(
                    schema, context=self.execution_context
                )
            except NotImplementedError as e:
                # ë¯¸êµ¬í˜„ ë…¸ë“œ íƒ€ì…ì— ëŒ€í•œ ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€
                raise NotImplementedError(
                    f"Cannot create node '{node_id}': {str(e)}"
                ) from e

    def _get_context(self, node_id: str, results: Dict) -> Dict[str, Any]:
        """
        í˜„ì¬ ë…¸ë“œê°€ ì‹¤í–‰ì— í•„ìš”í•œ ëª¨ë“  ì…ë ¥ ë°ì´í„°ë¥¼ êµ¬ì„±

        Returns:
            inputs: {
                # Node ID ë„¤ì„ìŠ¤í˜ì´ìŠ¤ (ëª…í™•ì„±)
                "node-a-id": {"key1": "value1"},
                "node-b-id": {"key2": "value2"}
            }

        íŠ¹ë³„ ì¼€ì´ìŠ¤:
            - StartNode: user_inputì„ ì§ì ‘ ì „ë‹¬ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì—†ì´)
        """
        # StartNode ë˜ëŠ” WebhookTriggerNode, ScheduleTriggerNodeëŠ” user_inputì„ ì§ì ‘ ë°›ìŒ
        node_schema = self.node_schemas.get(node_id)
        if node_schema and node_schema.type in [
            "startNode",
            "webhookTrigger",
            "scheduleTrigger",
        ]:
            return self.user_input

        # ì‹¤í–‰ëœ ëª¨ë“  ë…¸ë“œì˜ ê²°ê³¼ë¥¼ ì „ë‹¬ (ì¡°ìƒ ë…¸ë“œ ì°¸ì¡° ê°€ëŠ¥)
        # ì°¸ì¡°ë§Œ ì „ë‹¬í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ ë³µì‚¬ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
        return dict(results)

    def _get_answer_node_result(self, results: Dict) -> Dict[str, Any]:
        """
        ë°°í¬ ëª¨ë“œì—ì„œ AnswerNodeì˜ ê²°ê³¼ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            results: ëª¨ë“  ë…¸ë“œì˜ ì‹¤í–‰ ê²°ê³¼

        Returns:
            AnswerNodeì˜ ì‹¤í–‰ ê²°ê³¼

        Raises:
            ValueError: AnswerNodeë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
        """
        # [PERF] O(N) â†’ O(1) ê°œì„ : íƒ€ì…ë³„ ì¸ë±ìŠ¤ í™œìš©
        answer_nodes = self.nodes_by_type.get("answerNode", [])

        # ì‹¤í–‰ëœ ì²« ë²ˆì§¸ answerNode ì°¾ê¸°
        for node_id in answer_nodes:
            if node_id in results:
                return results[node_id]

        # ì‹¤í–‰ëœ AnswerNodeê°€ ì—†ëŠ” ê²½ìš°
        # raise ValueError(
        #     "ë°°í¬ëœ ì›Œí¬í”Œë¡œìš°ì—ëŠ” ì‹¤í–‰ëœ AnswerNodeê°€ í•„ìš”í•©ë‹ˆë‹¤. "
        #     "ì¡°ê±´ ë¶„ê¸°ë¡œ ì¸í•´ AnswerNodeê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ê±°ë‚˜, AnswerNodeê°€ ì›Œí¬í”Œë¡œìš°ì— ì—†ìŠµë‹ˆë‹¤."
        # )

    def _extract_node_options(self, node_schema) -> Dict[str, Any]:
        """
        ë…¸ë“œ ì„¤ì •ì„ process_dataìš© ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ì‹¤í–‰ ì‹œì ì˜ ë…¸ë“œ ì˜µì…˜ì„ ë¡œê·¸ì— ì €ì¥í•˜ì—¬ ë””ë²„ê¹…/ë¶„ì„ì— í™œìš©í•©ë‹ˆë‹¤.

        Args:
            node_schema: ë…¸ë“œ ìŠ¤í‚¤ë§ˆ (NodeSchema)

        Returns:
            ë…¸ë“œ ì˜µì…˜ ìŠ¤ëƒ…ìƒ· ë”•ì…”ë„ˆë¦¬
        """
        try:
            # node_schema.dataë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬
            data = dict(node_schema.data) if node_schema.data else {}

            return {
                "node_options": data,
                "node_title": data.get("title", ""),
            }
        except Exception:
            # ìŠ¤ëƒ…ìƒ· ì¶”ì¶œ ì‹¤íŒ¨ ì‹œì—ë„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ì€ ê³„ì†
            return {}
