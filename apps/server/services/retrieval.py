from sqlalchemy import select
from sqlalchemy.orm import Session
from langchain_openai import OpenAIEmbeddings

from db.models.knowledge import Document, DocumentChunk
from schemas.rag import ChunkPreview, RAGResponse
from services.llm_service import LLMService


# --- Constants ---
QUERY_EXPANSION_SYSTEM_PROMPT = """
You are a query optimizer for the AI service 'Moduly'.
Moduly is an AI workflow automation tool.

Your Task:
Rewrite the user's query to use correct official terminology mostly in English.
Keep the user's original intent clearly.
Output ONLY the rewritten query text.

Term Glossary:
- ëª¨ë“ˆë¦¬ -> Moduly
- ë™ -> RAG (Retrieval Augmented Generation)
- ì›Œí¬í”Œë¡œìš° -> Workflow
"""

class RetrievalService:
    def __init__(self, db: Session):
        self.db = db
        # 1. LLM í´ë¼ì´ì–¸íŠ¸ í™•ë³´ (API Key íšë“ìš©)
        # TODO: ì‹¤ì œë¡œëŠ” ì‚¬ìš©ì IDë¥¼ ë°›ì•„ì™€ì„œ í•´ë‹¹ ì‚¬ìš©ìì˜ Providerë¥¼ ê°€ì ¸ì™€ì•¼ í•¨ (í˜„ì¬ëŠ” Shared Mode)
        try:
            self.llm_client = LLMService.get_any_provider_client(db)
            self.api_key = self.llm_client.api_key  # OpenAIClientì¸ ê²½ìš° api_key ì†ì„± ì¡´ì¬
        except Exception as e:
            print(f"[Retrieval] Failed to load LLM Client: {e}")
            self.llm_client = None
            self.api_key = None


    def search_documents(self, query: str, top_k: int = 5, threshold: float = 0.3) -> list[ChunkPreview]:
        """
        [Public API] ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ (Vector Search Only)
        ë‹¤ë¥¸ ë…¸ë“œ(ì˜ˆ: Knowledge Node)ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ë§Œ í•„ìš”í•  ë•Œ ì´ í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì„¸ìš”.
        """
        # 0. Query Expansion (Smart Search)
        # í•œê¸€ ë°œìŒ(ëª¨ë“ˆë¦¬) -> ì˜ì–´ í‚¤ì›Œë“œ(Moduly) ë“±ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ
        try:
            expansion_prompt = [
                {"role": "system", "content": QUERY_EXPANSION_SYSTEM_PROMPT},
                {"role": "user", "content": query}
            ]
            # LLMì—ê²Œ ì¿¼ë¦¬ ìµœì í™” ìš”ì²­
            expanded_resp = self.llm_client.invoke(expansion_prompt)
            rewritten_query = expanded_resp["choices"][0]["message"]["content"].strip()
            print(f"[Search] ğŸ§  Smart Rewrite: '{query}' -> '{rewritten_query}'")
            
            # ê²€ìƒ‰ì–´ë¥¼ ë³€í™˜ëœ ê²ƒìœ¼ë¡œ êµì²´
            query = rewritten_query
        except Exception as e:
            print(f"Query Expansion Failed: {e}")

        # 1. Query Embedding (Real)
        try:
            embeddings_model = OpenAIEmbeddings(
                model="text-embedding-3-small",
                openai_api_key=self.api_key
            )
            query_vector = embeddings_model.embed_query(query)
        except Exception as e:
            print(f"Embedding Failed: {e}")
            return []

        # 2. Vector Search (SQLAlchemy with pgvector)
        # Select distance explicitly to filter by it
        distance_col = DocumentChunk.embedding.cosine_distance(query_vector).label("distance")
        
        stmt = (
            select(DocumentChunk, Document, distance_col)
            .join(Document)
            .order_by(distance_col)
            .limit(top_k)
        )

        results = self.db.execute(stmt).all()

        previews = []
        for chunk, doc, distance in results:
            # Similarity = 1 - distance
            similarity = 1 - distance
            
            if similarity < threshold:
                continue

            previews.append(
                ChunkPreview(
                    content=chunk.content,
                    document_id=doc.id,
                    filename=doc.filename,
                    page_number=chunk.metadata_.get("page"),
                    similarity_score=float(similarity),
                )
            )

        return previews

    def retrieve_context(self, query: str, top_k: int = 5) -> str:
        """
        [Public API] ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì³ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
        LLMì—ê²Œ í”„ë¡¬í”„íŠ¸ë¡œ ë„˜ê²¨ì¤„ Context ë©ì–´ë¦¬ê°€ í•„ìš”í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.
        """
        chunks = self.search_documents(query, top_k)
        if not chunks:
            return ""
        
        return "\n\n".join([c.content for c in chunks])

    def generate_answer(self, query: str) -> RAGResponse:
        """
        [Public API] ê²€ìƒ‰ + ë‹µë³€ ìƒì„± (Chat Interfaceìš©)
        """
        if not self.api_key:
            return RAGResponse(
                answer="âš ï¸ OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì‹¤ì œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                references=[]
            )

        # Step 1: Search (Reuse public method)
        relevant_chunks = self.search_documents(query)

        # Step 2: Context Construction
        if not relevant_chunks:
            return RAGResponse(
                answer="í•´ë‹¹ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                references=[]
            )

        context_text = "\n\n".join([c.content for c in relevant_chunks])

        # Step 3: LLM Generation
        system_prompt = (
            "You are a helpful assistant. Use the following context to answer the user's question.\n"
            "If the answer is not in the context, say you don't know.\n\n"
            f"Context:\n{context_text}"
        )
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": query}
        ]

        try:
            result = self.llm_client.invoke(messages)
            answer = result["choices"][0]["message"]["content"]
        except Exception as e:
            print(f"LLM Generation Failed: {e}")
            answer = f"ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({str(e)})"

        return RAGResponse(answer=answer, references=relevant_chunks)
