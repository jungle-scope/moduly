import os  # í´ë” ë§Œë“¤ê¸°ìš©
import shutil  # íŒŒì¼ ë³µì‚¬ìš©
from enum import Enum
from uuid import UUID

import fitz
import pymupdf4llm
import tiktoken
from fastapi import UploadFile
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from sqlalchemy.orm import Session

from db.models.knowledge import Document, DocumentChunk


class ParsingStrategy(str, Enum):
    TEXT = "text"
    MIXED = "mixed"
    IMAGE = "image"


class IngestionService:
    def __init__(
        self,
        db: Session,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        ai_model: str = "text-embedding-3-small",
    ):
        self.db = db
        self.ai_model = ai_model

        # ì²­í‚¹ ì „ëµ ì„¤ì •
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            # ë¬¸ë‹¨ ë°”ë€” ë•Œ, ì¤„ ë°”ê¿€ ë•Œ, ë§ˆì¹¨í‘œ, ë„ì–´ì“°ê¸°ì¼ ë•Œ ìë¥¸ë‹¤
            separators=["\n\n", "\n", ".", " ", ""],
            keep_separator=True,
        )

    def save_temp_file(self, file: UploadFile) -> str:
        """
        ì„¤ëª…: ë©”ëª¨ë¦¬ì— ìˆëŠ” ì—…ë¡œë“œ íŒŒì¼ì„ ë””ìŠ¤í¬(uploads í´ë”)ì— ì €ì¥í•©ë‹ˆë‹¤.
        """

        upload_dir = "uploads"
        os.makedirs(upload_dir, exist_ok=True)

        # ì €ì¥ë  íŒŒì¼ì˜ ì „ì²´ ì£¼ì†Œ (ì˜ˆ: "uploads/ë³´ê³ ì„œ.pdf")
        file_path = os.path.join(upload_dir, file.filename)

        with open(file_path, "wb") as buffer:
            # ë©”ëª¨ë¦¬ì— ìˆëŠ” íŒŒì¼(file.file)ì„ í•˜ë“œë””ìŠ¤í¬(buffer)ë¡œ ë³µì‚¬
            shutil.copyfileobj(file.file, buffer)

        return file_path

    def create_pending_document(
        self,
        knowledge_base_id: UUID,
        filename: str,
        file_path: str,
        chunk_size: int,
        chunk_overlap: int,
    ) -> UUID:
        """
        íŒŒì¼ ì—…ë¡œë“œ ì‹œì ì— 'Pending' ìƒíƒœì˜ Document ë ˆì½”ë“œë¥¼ ë¨¼ì € ìƒì„±í•©ë‹ˆë‹¤.
        KnowledgeBaseì™€ì˜ ì—°ê²°(FK)ì„ ìœ„í•´ knowledge_base_idê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.
        ì„¤ì •ëœ chunk_sizeì™€ chunk_overlapì„ ì €ì¥í•˜ì—¬ ë‚˜ì¤‘ì— ì°¸ì¡°í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
        """
        new_doc = Document(
            knowledge_base_id=knowledge_base_id,
            filename=filename,
            file_path=file_path,
            status="pending",
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
        )
        self.db.add(new_doc)
        self.db.commit()
        self.db.refresh(new_doc)
        return new_doc.id

    async def process_document_background(
        self, document_id: UUID, knowledge_base_id: UUID, file_path: str
    ):
        """
        BackgroundTasksì˜ ë©”ì¸ ì§„ì…ì .
        íŒŒì‹± -> ì²­í‚¹ -> ì„ë² ë”© -> ì €ì¥
        """
        try:
            self._update_status(document_id, "indexing")

            # 1ë‹¨ê³„: íŒŒì‹± (document_id ì „ë‹¬)
            text_blocks = self._parse_pdf(file_path, document_id)

            # íŒŒì‹± ê²°ê³¼ê°€ ë¹„ì–´ìˆë‹¤ë©´ (ë¹„ìš© ìŠ¹ì¸ ëŒ€ê¸° ë“±) ì¤‘ë‹¨
            if not text_blocks:
                doc = self.db.query(Document).get(document_id)
                if doc and doc.status == "waiting_for_approval":
                    print(f"â¸ï¸ Document {document_id} paused for approval.")
                    return
                # ì§„ì§œ ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš°ì¼ ìˆ˜ë„ ìˆìŒ (ì´ ê²½ìš° completed ì²˜ë¦¬ë¨)

            # 2ë‹¨ê³„: ì²­í‚¹
            chunks = self._create_chunks(text_blocks)

            # 3 & 4ë‹¨ê³„: ì„ë² ë”© ë° ì €ì¥
            self._save_chunks_to_pgvector(document_id, knowledge_base_id, chunks)

            self._update_status(document_id, "completed")
        except Exception as e:
            print(f"Ingestion failed: {e}")
            self._update_status(document_id, "failed", error_message=str(e))

    async def resume_with_llamaparse(self, document_id: UUID):
        """
        ìŠ¹ì¸ëœ ë¬¸ì„œì— ëŒ€í•´ LlamaParseë¡œ íŒŒì‹±ì„ ì¬ê°œí•˜ê³  ë‚˜ë¨¸ì§€ íŒŒì´í”„ë¼ì¸ ìˆ˜í–‰
        """
        doc = self.db.query(Document).get(document_id)
        if not doc:
            print(f"âŒ Document {document_id} not found for resumption.")
            return

        try:
            print(f"â–¶ï¸ Resuming ingestion for {document_id} with LlamaParse...")
            self._update_status(document_id, "indexing")

            # 1ë‹¨ê³„: LlamaParse ê°•ì œ ì‹¤í–‰
            text_blocks = self._parse_with_llamaparse(doc.file_path)

            # 2ë‹¨ê³„: ì²­í‚¹
            chunks = self._create_chunks(text_blocks)

            # 3 & 4ë‹¨ê³„: ì„ë² ë”© ë° ì €ì¥
            self._save_chunks_to_pgvector(document_id, doc.knowledge_base_id, chunks)

            self._update_status(document_id, "completed")

        except Exception as e:
            print(f"âŒ Resumption failed: {e}")
            self._update_status(document_id, "failed", error_message=str(e))

    def _analyze_pdf_type(self, file_path: str) -> str:
        """
        PDF íŒŒì¼ì˜ ì„±ê²©ì„ íŒŒì•…í•˜ì—¬ ì ì ˆí•œ íŒŒì‹± ì „ëµì„ ë°˜í™˜í•©ë‹ˆë‹¤.

        Sampling Strategy:
        - ì• 3í˜ì´ì§€ + ì¤‘ê°„ 1í˜ì´ì§€ + ë’¤ 2í˜ì´ì§€ (ì´ ìµœëŒ€ 6í˜ì´ì§€)

        Returns:
            - 'special': ì „ì²´ê°€ ì´ë¯¸ì§€ê±°ë‚˜ í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ëŠ” ê²½ìš° (LlamaParse ë“± í•„ìš”) -> OCR í•„ìš”
            - 'fast': í…ìŠ¤íŠ¸ ìœ„ì£¼ì˜ ì¼ë°˜ì ì¸ ë¬¸ì„œ (PyMuPDF4LLM ì‚¬ìš©)
            - 'precise': í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ê°€ ì„ì—¬ìˆì–´ ì •ë°€í•œ ë ˆì´ì•„ì›ƒ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°
        """
        doc = fitz.open(file_path)
        total_pages = len(doc)

        # 1. ë„ˆë¬´ í° íŒŒì¼ ì˜ˆì™¸ ì²˜ë¦¬ (ì˜ˆ: 500í˜ì´ì§€ ì´ìƒì€ ì¼ë‹¨ ê²½ê³ )
        if total_pages > 500:
            print(f"[Warn] Large file detected: {total_pages} pages.")

        # 2. ìƒ˜í”Œë§ í˜ì´ì§€ ì¸ë±ìŠ¤ ì„ ì •
        sample_indices = set()

        # ì• 3í˜ì´ì§€
        for i in range(min(3, total_pages)):
            sample_indices.add(i)

        # ì¤‘ê°„ 1í˜ì´ì§€
        if total_pages > 3:
            sample_indices.add(total_pages // 2)

        # ë’¤ 2í˜ì´ì§€
        if total_pages > 1:
            sample_indices.add(total_pages - 1)
        if total_pages > 2:
            sample_indices.add(total_pages - 2)

        sorted_indices = sorted(list(sample_indices))

        # 3. ìƒ˜í”Œë§ ë¶„ì„
        image_count = 0
        text_length = 0
        page_count = 0

        for idx in sorted_indices:
            if idx >= total_pages:
                continue

            page = doc[idx]
            page_count += 1

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = page.get_text()
            text_length += len(text.strip())

            # ì´ë¯¸ì§€ ê°ì²´ ì¹´ìš´íŠ¸
            images = page.get_images(full=True)
            image_count += len(images)

        doc.close()

        # 4. ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ì „ëµ ê²°ì •

        # í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´ (í˜ì´ì§€ë‹¹)
        avg_text_per_page = text_length / page_count if page_count > 0 else 0

        # í‰ê·  ì´ë¯¸ì§€ ìˆ˜ (í˜ì´ì§€ë‹¹)
        avg_images_per_page = image_count / page_count if page_count > 0 else 0

        print(
            f"[PDF Analysis] Avg Text: {avg_text_per_page:.1f}, Avg Images: {avg_images_per_page:.1f}"
        )

        # Case A: í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ìŒ (OCR í•„ìš”)
        if avg_text_per_page < 50:
            return ParsingStrategy.IMAGE

        # Case B: ì´ë¯¸ì§€ê°€ ë§ê³  í…ìŠ¤íŠ¸ë„ ì–´ëŠì •ë„ ìˆìŒ (ë³µì¡í•œ ë ˆì´ì•„ì›ƒ ê°€ëŠ¥ì„±)
        elif avg_images_per_page > 2:
            return ParsingStrategy.MIXED

        # Case C: í…ìŠ¤íŠ¸ ìœ„ì£¼
        else:
            return ParsingStrategy.TEXT

    def _parse_with_pymupdf(self, file_path: str) -> list[dict]:
        """ê¸°ì¡´ PyMuPDF4LLM ê¸°ë°˜ íŒŒì‹± ë¡œì§"""
        md_text_chunks = pymupdf4llm.to_markdown(file_path, page_chunks=True)

        results = []
        for chunk in md_text_chunks:
            results.append(
                {
                    "text": chunk["text"],
                    "page": chunk["metadata"]["page"] + 1,
                }
            )
        return results

    def _parse_with_llamaparse(self, file_path: str) -> list[dict]:
        """LlamaParse API ì—°ë™"""

        # ë¹„ìš© ì˜ˆì¸¡ ë¡œê·¸ ì¶œë ¥
        est = self._estimate_llamaparse_cost(file_path)
        print(
            f"ğŸ’° [ë¹„ìš© ì˜ˆì¸¡] í˜ì´ì§€ ìˆ˜: {est['pages']}, í¬ë ˆë”§: {est['credits']}, ë¹„ìš©: ${est['cost_usd']:.4f}"
        )

        try:
            from llama_parse import LlamaParse
        except ImportError:
            print(
                "âŒ LlamaParse ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'pip install llama-parse'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
            )
            return []

        api_key = os.getenv("LLAMA_CLOUD_API_KEY")
        if not api_key:
            print("âŒ LLAMA_CLOUD_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        print("ğŸš€ LlamaParse í´ë¼ìš°ë“œ ì²˜ë¦¬ ì‹œì‘...")

        try:
            # íŒŒì„œ ì´ˆê¸°í™”
            # result_type="markdown"ì´ ê¸°ë³¸ê°’ì´ì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
            # language="ko"ë¥¼ ì„¤ì •í•˜ì—¬ í•œêµ­ì–´ ì¸ì‹ë¥  í–¥ìƒ
            parser = LlamaParse(
                api_key=api_key,
                result_type="markdown",
                verbose=True,
                language="ko",
                fast_mode=True,
            )

            # JSON ê²°ê³¼ë¥¼ ë°›ì•„ì•¼ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ í™•ì‹¤í•˜ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆìŒ
            # get_json_resultëŠ” íŒŒì¼ë‹¹ í•˜ë‚˜ì˜ ê²°ê³¼ ê°ì²´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•¨
            json_results = parser.get_json_result(file_path)

            # [Debug] êµ¬ì¡° í™•ì¸
            # print(f"ğŸ” [LlamaParse Raw Result]: {json_results}")

            parsed_results = []
            full_text_for_debug = ""

            if json_results and isinstance(json_results, list):
                first_result = json_results[0]
                # 'pages' í‚¤ì— ê° í˜ì´ì§€ë³„ íŒŒì‹± ê²°ê³¼ê°€ ë‹´ê²¨ìˆìŒ
                pages = first_result.get("pages", [])

                for p in pages:
                    # 'md' í‚¤ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ í‚¤ í™•ì¸
                    # 'md' í‚¤ê°€ ì—†ìœ¼ë©´ 'text' í‚¤ë¥¼ ì‚¬ìš© (fast_mode ë“±ì—ì„œ ë°œìƒ)
                    md_text = p.get("md") or p.get("text") or ""
                    parsed_results.append(
                        {
                            "text": md_text,  # ë§ˆí¬ë‹¤ìš´ ë³€í™˜ í…ìŠ¤íŠ¸
                            "page": p["page"],  # í˜ì´ì§€ ë²ˆí˜¸
                        }
                    )
                    full_text_for_debug += f"\n--- Page {p['page']} ---\n{md_text}\n"

            # [Debug] íŒŒì‹±ëœ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            # try:
            #     base_dir = os.path.dirname(file_path)
            #     file_name = os.path.basename(file_path)
            #     debug_file_name = f"{os.path.splitext(file_name)[0]}_parsed.md"
            #     debug_file_path = os.path.join(base_dir, debug_file_name)

            #     with open(debug_file_path, "w", encoding="utf-8") as f:
            #         f.write(full_text_for_debug)
            #     print(f"ğŸ’¾ [Debug] Parsed content saved to: {debug_file_path}")
            # except Exception as e:
            #     print(f"âš ï¸ Failed to save debug file: {e}")

            print(f"LlamaParse ì™„ë£Œ: ì´ {len(parsed_results)} í˜ì´ì§€ ë³€í™˜ë¨.")
            return parsed_results

        except Exception as e:
            print(f"LlamaParse ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []

    def _estimate_llamaparse_cost(self, file_path: str) -> dict:
        """
        LlamaParse ì˜ˆì¸¡ ë¹„ìš© ê³„ì‚°
        ê¸°ì¤€: Standard Mode (3 credits/page), $1 = 1000 credits
        """
        try:
            doc = fitz.open(file_path)
            total_pages = len(doc)
            doc.close()

            # Standard Mode ê¸°ì¤€ (í˜ì´ì§€ë‹¹ 3 í¬ë ˆë”§)
            credits_per_page = 3
            total_credits = total_pages * credits_per_page
            cost_usd = total_credits / 1000.0

            return {
                "pages": total_pages,
                "credits": total_credits,
                "cost_usd": cost_usd,
            }
        except Exception as e:
            print(f"[Warning] Cost estimation failed: {e}")
            return {"pages": 0, "credits": 0, "cost_usd": 0.0}

    def _is_mixed_quality_poor(self, results: list[dict]) -> bool:
        """MIXED ëª¨ë“œ í’ˆì§ˆ ê²€ì‚¬: ë ˆì´ì•„ì›ƒì´ ì‹¬ê°í•˜ê²Œ ê¹¨ì¡ŒëŠ”ì§€ í™•ì¸"""
        total_text = "".join([r["text"] for r in results])

        # íœ´ë¦¬ìŠ¤í‹± 1: ì•Œ ìˆ˜ ì—†ëŠ” íŠ¹ìˆ˜ë¬¸ìë‚˜ ê³µë°± íŒ¨í„´ì´ ë„ˆë¬´ ë§ì€ ê²½ìš°
        if len(total_text) > 0:
            broken_char_count = total_text.count("\ufffd")
            if (broken_char_count / len(total_text)) > 0.05:  # 5% ì´ìƒ ê¹¨ì§
                print("Reason: High broken character rate in MIXED mode.")
                return True

        # íœ´ë¦¬ìŠ¤í‹± 2: ë§ˆí¬ë‹¤ìš´ êµ¬ì¡°ê°€ ê±°ì˜ ì—†ìŒ (í—¤ë” #ì´ ë„ˆë¬´ ì ìŒ)
        # ì¼ë°˜ì ì¸ ë¬¸ì„œë¼ë©´ í˜ì´ì§€ë‹¹ ì ì–´ë„ 1~2ê°œì˜ í—¤ë”ëŠ” ìˆì–´ì•¼ í•¨
        page_count = len(results)
        header_count = total_text.count("\n#")
        if (
            page_count > 0 and (header_count / page_count) < 0.2
        ):  # 5í˜ì´ì§€ë‹¹ í—¤ë” 1ê°œ ë¯¸ë§Œ
            print("Reason: Too few markdown headers found.")
            return True

        return False

    def _is_text_quality_poor(self, file_path: str, results: list[dict]) -> bool:
        """TEXT ëª¨ë“œ í’ˆì§ˆ ê²€ì‚¬"""
        total_text = "".join([r["text"] for r in results])

        # 1. ê¸€ì ìˆ˜ê°€ ë„ˆë¬´ ì ìŒ (50ì ë¯¸ë§Œ)
        if len(total_text.strip()) < 50:
            print("Reason: Too few characters extracted.")
            return True

        # 2. ê¹¨ì§„ ë¬¸ì(replacement character ) ë¹„ìœ¨ í™•ì¸
        broken_char_count = total_text.count("\ufffd")  # or other garbage chars
        if len(total_text) > 0 and (
            broken_char_count / len(total_text) > 0.05
        ):  # 5% ì´ìƒ
            print("Reason: Too many broken characters.")
            return True

        # 3. (ê³ ê¸‰) PyMuPDFë¡œ í‘œ(Table)ëŠ” ê°ì§€ë˜ëŠ”ë°, ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì—ëŠ” ë§ˆí¬ë‹¤ìš´ í‘œ ë¬¸ë²•(|---|)ì´ ì—†ëŠ” ê²½ìš°
        try:
            doc = fitz.open(file_path)
            has_table_but_no_markdown = False

            # ì„±ëŠ¥ì„ ìœ„í•´ ì•ë¶€ë¶„ 5í˜ì´ì§€ë§Œ ê²€ì‚¬
            for i in range(min(5, len(doc))):
                page = doc[i]
                tables = page.find_tables()
                if tables and len(tables.tables) > 0:
                    # í•´ë‹¹ í˜ì´ì§€ì˜ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì°¾ê¸°
                    page_text = results[i]["text"] if i < len(results) else ""
                    # í‘œëŠ” ìˆëŠ”ë° ë§ˆí¬ë‹¤ìš´ í‘œ êµ¬ë¬¸('|')ì´ ì „í˜€ ì—†ë‹¤ë©´ íŒŒì‹± ì‹¤íŒ¨ë¡œ ê°„ì£¼
                    if "|" not in page_text:
                        print(
                            f"Reason: Table detected on page {i + 1} but no markdown table found."
                        )
                        has_table_but_no_markdown = True
                        break

            doc.close()
            if has_table_but_no_markdown:
                return True

        except Exception as e:
            print(f"[Warning] Table check failed: {e}")
            # ì—ëŸ¬ ë‚˜ë©´ ì•ˆì „í•˜ê²Œ False ë°˜í™˜ (Flow ì¤‘ë‹¨ ì•ˆ í•¨)
            return False

        return False

    def _request_llamaparse_approval(
        self, file_path: str, document_id: UUID
    ) -> list[dict]:
        """
        LlamaParse í˜¸ì¶œ ì „ ë¹„ìš© ê³„ì‚° í›„ 'ìŠ¹ì¸ ëŒ€ê¸°' ìƒíƒœë¡œ ë³€ê²½í•˜ê³  ì¤‘ë‹¨í•¨.
        """
        if not document_id:
            # document_idê°€ ì—†ìœ¼ë©´(ë””ë²„ê·¸/í…ŒìŠ¤íŠ¸ ëª¨ë“œ) ê·¸ëƒ¥ ì§„í–‰
            print("No document_id provided. Skipping approval and running LlamaParse.")
            return self._parse_with_llamaparse(file_path)

        # 1. ë¹„ìš© ê³„ì‚°
        est = self._estimate_llamaparse_cost(file_path)

        # 2. DB ì—…ë°ì´íŠ¸ (ìƒíƒœ: waiting_for_approval)
        doc = self.db.query(Document).get(document_id)
        if doc:
            doc.status = "waiting_for_approval"
            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ì— ë¹„ìš© ì •ë³´ ë³‘í•©
            new_meta = dict(doc.meta_info or {})
            new_meta.update({"cost_estimate": est, "strategy": "llamaparse_fallback"})
            doc.meta_info = new_meta
            self.db.commit()

        print(
            f"â¸ï¸ [Approval Required] Document {document_id} paused for LlamaParse cost approval."
        )

        # 3. ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜í•˜ì—¬ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨
        return []

    def _parse_pdf(self, file_path: str, document_id: UUID = None) -> list[dict]:
        """
        PDF íŒŒì‹± ë©”ì¸ ì§„ì…ì .
        ì ì ˆí•œ íŒŒì„œ(PyMuPDF / LlamaParse)ë¥¼ ì„ íƒí•˜ê³ ,
        í’ˆì§ˆ ì €í•˜ ì‹œ Fallback ë¡œì§ì„ ìˆ˜í–‰ (ë¹„ìš© ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ í¬í•¨)
        """
        # 1. íŒŒì¼ ì„±ê²© íŒŒì•…
        parsing_strategy = self._analyze_pdf_type(file_path)
        print(f"[{file_path}] Parsing Strategy: {parsing_strategy.value}")

        # Case 1: ì´ë¯¸ì§€ ìœ„ì£¼ (OCR í•„ìˆ˜) -> ìŠ¹ì¸ ìš”ì²­
        if parsing_strategy == ParsingStrategy.IMAGE:
            print("Strategy is IMAGE. Requesting approval for LlamaParse.")
            return self._request_llamaparse_approval(file_path, document_id)

        # Case 2: í˜¼í•©í˜• (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)
        elif parsing_strategy == ParsingStrategy.MIXED:
            # 1ì°¨ ì‹œë„: PyMuPDF (ë¹ ë¦„)
            results = self._parse_with_pymupdf(file_path)

            # í’ˆì§ˆ ê²€ì‚¬: ê²°ê³¼ë¬¼ì´ 'ë‚œì¡'í•œì§€ í™•ì¸
            if self._is_mixed_quality_poor(results):
                print(
                    "Mixed parsing quality is poor. Requesting approval for LlamaParse."
                )
                return self._request_llamaparse_approval(file_path, document_id)

            return results

        # Case 3: í…ìŠ¤íŠ¸ ìœ„ì£¼
        else:  # ParsingStrategy.TEXT
            # 1ì°¨ ì‹œë„: PyMuPDF
            results = self._parse_with_pymupdf(file_path)

            # í’ˆì§ˆ ê²€ì‚¬: í…ìŠ¤íŠ¸ ëˆ„ë½, ê¹¨ì§, í‘œ êµ¬ì¡° ì´ìƒ í™•ì¸
            if self._is_text_quality_poor(file_path, results):
                print(
                    "Text parsing quality is poor. Requesting approval for LlamaParse."
                )
                return self._request_llamaparse_approval(file_path, document_id)

            return results

    def _create_chunks(self, text_blocks: list[dict]) -> list[dict]:
        """
        íŒŒì‹±ëœ í…ìŠ¤íŠ¸ë¥¼ ë” ì‘ì€ ì¡°ê°(Chunk)ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
        """
        final_chunks = []

        for block in text_blocks:
            splits = self.text_splitter.split_text(block["text"])
            for split in splits:
                final_chunks.append(
                    {"content": split, "metadata": {"page": block["page"]}}
                )
        return final_chunks

    def _save_chunks_to_pgvector(
        self, document_id: UUID, knowledge_base_id: UUID, chunks: list[dict]
    ):
        """
        í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ OpenAIì— ë³´ë‚´ì„œ 'ì˜ë¯¸ ë²¡í„°'ë¡œ ë°”ê¾¼ ë’¤, DocumentChunk í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤.
        """
        # í† í° ê³„ì‚°ì„ ìœ„í•œ ì¸ì½”ë” ì„¤ì •
        try:
            encoding = tiktoken.encoding_for_model(self.ai_model)
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base")  # gpt-4ë¡œ ê°€ì •í•˜ê³  ê³„ì‚°

        # DBì—ì„œ API Key ê°€ì ¸ì˜¤ê¸° (í™˜ê²½ë³€ìˆ˜ ì˜ì¡´ ì œê±°)
        from services.llm_service import LLMService

        api_key = LLMService.get_default_api_key(self.db)

        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (API Key ëª…ì‹œ)
        embeddings_model = OpenAIEmbeddings(model=self.ai_model, openai_api_key=api_key)

        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´)
        texts = [chunk["content"] for chunk in chunks]

        # 2. ì„ë² ë”© ìƒì„± (ì¼ê´„ í˜¸ì¶œ) - ì‹¤ì œ API ì‚¬ìš©!
        try:
            embedded_vectors = embeddings_model.embed_documents(texts)
        except Exception as e:
            print(f"OpenAI Embedding Error: {e}")
            raise e

        # 3. DB ê°ì²´ ìƒì„±
        chunk_objects = []
        for i, chunk in enumerate(chunks):
            content = chunk["content"]
            token_count = len(encoding.encode(content))

            db_chunk = DocumentChunk(
                document_id=document_id,
                knowledge_base_id=knowledge_base_id,  # ê²€ìƒ‰ ìµœì í™”ìš©
                content=content,
                embedding=embedded_vectors[i],
                chunk_index=i,
                token_count=token_count,
                metadata_=chunk["metadata"],
            )
            chunk_objects.append(db_chunk)

        self.db.add_all(chunk_objects)
        self.db.commit()

    def _update_status(self, document_id: UUID, status: str, error_message: str = None):
        doc = self.db.query(Document).get(document_id)
        if doc:
            doc.status = status
            if error_message:
                doc.error_message = error_message
            self.db.commit()
