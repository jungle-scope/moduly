import hashlib
import json
import os  # í´ë” ë§Œë“¤ê¸°ìš©
import re
import shutil  # íŒŒì¼ ë³µì‚¬ìš©
from enum import Enum
from typing import Optional
from uuid import UUID

import fitz  # PyMuPDF
import pandas as pd
import pymupdf4llm
import tiktoken
from docx import Document as DocxDocument
from fastapi import UploadFile
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from sqlalchemy.orm import Session

from db.models.knowledge import Document, DocumentChunk, SourceType
from db.models.llm import LLMCredential, LLMProvider
from services.data_sources import ApiDataSource, BaseDataSource, FileDataSource


class ParsingStrategy(str, Enum):
    TEXT = "text"
    MIXED = "mixed"
    IMAGE = "image"


class IngestionService:
    def __init__(
        self,
        db: Session,
        user_id: Optional[UUID] = None,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        ai_model: str = "text-embedding-3-small",
    ):
        self.db = db
        self.user_id = user_id
        self.ai_model = ai_model

        # ì²­í‚¹ ì „ëµ ì„¤ì •
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            # ë¬¸ë‹¨ ë°”ë€” ë•Œ, ì¤„ ë°”ê¿€ ë•Œ, ë§ˆì¹¨í‘œ, ë„ì–´ì“°ê¸°ì¼ ë•Œ ìë¥¸ë‹¤
            separators=["\n\n", "\n", ".", " ", ""],
            keep_separator=True,
        )

    def _get_data_source(self, source_type: str) -> BaseDataSource:
        if source_type == SourceType.FILE:
            return FileDataSource()
        elif source_type == SourceType.API:
            return ApiDataSource()
        # Default fallback for legacy data or if source_type is string "FILE"
        if str(source_type) == "FILE":
            return FileDataSource()
        raise ValueError(f"Unknown source type: {source_type}")

    def save_temp_file(self, file: UploadFile) -> str:
        """
        ì„¤ëª…: ë©”ëª¨ë¦¬ì— ìˆëŠ” ì—…ë¡œë“œ íŒŒì¼ì„ ë””ìŠ¤í¬(uploads í´ë”)ì— ì €ì¥í•©ë‹ˆë‹¤.
        ë™ì¼í•œ íŒŒì¼ëª…ì´ ì—…ë¡œë“œë˜ì–´ë„ ë¬¼ë¦¬ì  ì¶©ëŒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ UUIDë¥¼ ë¶™ì—¬ì„œ ì €ì¥í•©ë‹ˆë‹¤.
        """
        import uuid

        upload_dir = "uploads"
        os.makedirs(upload_dir, exist_ok=True)

        # ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„± (ì˜ˆ: a1b2c3d4..._ë³´ê³ ì„œ.pdf)
        unique_filename = f"{uuid.uuid4()}_{file.filename}"

        # ì €ì¥ë  íŒŒì¼ì˜ ì „ì²´ ì£¼ì†Œ (ì˜ˆ: "uploads/a1b2c3d4..._ë³´ê³ ì„œ.pdf")
        file_path = os.path.join(upload_dir, unique_filename)

        with open(file_path, "wb") as buffer:
            # ë©”ëª¨ë¦¬ì— ìˆëŠ” íŒŒì¼(file.file)ì„ í•˜ë“œë””ìŠ¤í¬(buffer)ë¡œ ë³µì‚¬
            shutil.copyfileobj(file.file, buffer)

        return file_path

    def create_pending_document(
        self,
        knowledge_base_id: UUID,
        filename: str,
        file_path: str | None,
        chunk_size: int,
        chunk_overlap: int,
        source_type: SourceType = SourceType.FILE,
        meta_info: dict = None,
    ) -> UUID:
        """
        íŒŒì¼ ì—…ë¡œë“œ ì‹œì ì— 'Pending' ìƒíƒœì˜ Document ë ˆì½”ë“œë¥¼ ë¨¼ì € ìƒì„±í•©ë‹ˆë‹¤.
        KnowledgeBaseì™€ì˜ ì—°ê²°(FK)ì„ ìœ„í•´ knowledge_base_idê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.
        ì„¤ì •ëœ chunk_sizeì™€ chunk_overlapì„ ì €ì¥í•˜ì—¬ ë‚˜ì¤‘ì— ì°¸ì¡°í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
        """
        new_doc = Document(
            knowledge_base_id=knowledge_base_id,
            filename=filename,
            file_path=file_path,
            status="pending",
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            source_type=source_type,
            meta_info=meta_info or {},
        )
        self.db.add(new_doc)
        self.db.commit()
        self.db.refresh(new_doc)
        return new_doc.id

    async def process_document_background(
        self, document_id: UUID, knowledge_base_id: UUID, file_path: str
    ):
        """
        BackgroundTasksì˜ ë©”ì¸ ì§„ì…ì .
        íŒŒì‹± -> ì²­í‚¹ -> ì„ë² ë”© -> ì €ì¥
        """
        try:
            self._update_status(document_id, "indexing")
            self._update_progress(document_id, 5, "ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            print("[DEBUG] 1ë²ˆ")
            # 1ë‹¨ê³„: íŒŒì‹± (document_id ì „ë‹¬)
            self._update_progress(document_id, 10, "ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")

            # 1.3 DataSourceë¥¼ í†µí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            # DBì—ì„œ Document ê°ì²´ ì¡°íšŒ
            doc = self.db.query(Document).get(document_id)
            if not doc:
                raise ValueError(f"Document {document_id} not found")

            data_source = self._get_data_source(doc.source_type)
            print("[DEBUG] 2ë²ˆ", data_source)

            # ì†ŒìŠ¤ ì„¤ì • êµ¬ì„±
            source_config = {}
            if doc.source_type == SourceType.FILE or str(doc.source_type) == "FILE":
                source_config = {
                    "file_path": file_path,
                    "document_id": str(document_id),
                }
            elif doc.source_type == SourceType.API:
                # meta_infoì—ì„œ API ì„¤ì • ê°€ì ¸ì˜¤ê¸°
                api_config = doc.meta_info.get("api_config", {})

                # í—¤ë” ë³µí˜¸í™” ë¡œì§
                import json

                from core.security import security_service

                headers = api_config.get("headers")
                if headers and isinstance(headers, str):
                    try:
                        decrypted_json = security_service.decrypt(headers)
                        api_config["headers"] = json.loads(decrypted_json)
                    except Exception as e:
                        print(f"Failed to decrypt headers: {e}")
                        # ë³µí˜¸í™” ì‹¤íŒ¨ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©í•˜ê±°ë‚˜ ì—ëŸ¬ ì²˜ë¦¬
                        api_config["headers"] = {}

                source_config = api_config

            text_blocks = data_source.fetch_text(source_config)

            # íŒŒì‹± ê²°ê³¼ê°€ ë¹„ì–´ìˆë‹¤ë©´ (ë¹„ìš© ìŠ¹ì¸ ëŒ€ê¸° ë“±) ì¤‘ë‹¨
            if not text_blocks:
                doc = self.db.query(Document).get(document_id)
                if doc and doc.status == "waiting_for_approval":
                    print(f"â¸ï¸ Document {document_id} paused for approval.")
                    self._update_progress(
                        document_id, 0, "ì¶”ê°€ ë¹„ìš© ìŠ¹ì¸ì´ í•„ìš”í•˜ì—¬ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤."
                    )
                    return
                # ì§„ì§œ ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš°ì¼ ìˆ˜ë„ ìˆìŒ (ì´ ê²½ìš° completed ì²˜ë¦¬ë¨)
                print(f"âš ï¸ No text extracted from document {document_id}")
                self._update_status(document_id, "completed")
                return

            # 1.5 Content Hash Check (ë³€ê²½ ê°ì§€)
            # ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í•©ì³ì„œ í•´ì‹œ ìƒì„±
            full_text = "".join([b["text"] for b in text_blocks])
            new_hash = hashlib.sha256(full_text.encode("utf-8")).hexdigest()

            if doc.content_hash == new_hash:
                print(f"â­ï¸ Content unchanged for {document_id}. Skipping processing.")
                self._update_progress(
                    document_id, 100, "ë³€ê²½ëœ ë‚´ìš©ì´ ì—†ì–´ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤."
                )
                self._update_status(document_id, "completed")
                return

            # í•´ì‹œ ì—…ë°ì´íŠ¸
            doc = self.db.query(Document).get(document_id)
            if doc:
                doc.content_hash = new_hash
                self.db.commit()

            self._update_progress(document_id, 40, "ë¬¸ì„œ ë‚´ìš© ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

            # 2~4ë‹¨ê³„: ì²­í‚¹, ì„ë² ë”©, ì €ì¥ ë° ì™„ë£Œ ì²˜ë¦¬
            self._finalize_ingestion(document_id, knowledge_base_id, text_blocks)
        except Exception as e:
            print(f"Ingestion failed: {e}")
            self._update_status(document_id, "failed", error_message=str(e))
            self._update_progress(
                document_id, 0, f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            )

    async def resume_processing(self, document_id: UUID, strategy: str = "llamaparse"):
        """
        ìŠ¹ì¸ëœ ë¬¸ì„œì— ëŒ€í•´ íŒŒì‹±ì„ ì¬ê°œí•©ë‹ˆë‹¤.
        strategy: 'llamaparse' (ìœ ë£Œ, ê³ í’ˆì§ˆ) or 'general' (ë¬´ë£Œ, PyMuPDF)
        """
        doc = self.db.query(Document).get(document_id)
        if not doc:
            print(f"âŒ Document {document_id} not found for resumption.")
            return

        try:
            print(f"â–¶ï¸ Resuming ingestion for {document_id} with strategy: {strategy}")
            self._update_status(document_id, "indexing")

            text_blocks = []

            # 1ë‹¨ê³„: ì „ëµì— ë”°ë¥¸ íŒŒì‹± (DataSource ì‚¬ìš©)
            data_source = self._get_data_source(doc.source_type)

            source_config = {}
            if doc.source_type == SourceType.FILE:
                source_config = {
                    "file_path": doc.file_path,
                    "document_id": str(document_id),
                    "strategy": strategy,  # "general" or "llamaparse" passed from arg
                }
            elif doc.source_type == SourceType.API:
                api_config = doc.meta_info.get("api_config", {})
                source_config = api_config

            text_blocks = data_source.fetch_text(source_config)

            # 2~4ë‹¨ê³„: ì²­í‚¹, ì„ë² ë”©, ì €ì¥ ë° ì™„ë£Œ ì²˜ë¦¬
            self._finalize_ingestion(document_id, doc.knowledge_base_id, text_blocks)

        except Exception as e:
            print(f"âŒ Resumption failed: {e}")
            self._update_status(document_id, "failed", error_message=str(e))

    def _analyze_pdf_type(self, file_path: str) -> str:
        """
        PDF íŒŒì¼ì˜ ì„±ê²©ì„ íŒŒì•…í•˜ì—¬ ì ì ˆí•œ íŒŒì‹± ì „ëµì„ ë°˜í™˜í•©ë‹ˆë‹¤.

        Sampling Strategy:
        - ì• 3í˜ì´ì§€ + ì¤‘ê°„ 1í˜ì´ì§€ + ë’¤ 2í˜ì´ì§€ (ì´ ìµœëŒ€ 6í˜ì´ì§€)

        Returns:
            - 'special': ì „ì²´ê°€ ì´ë¯¸ì§€ê±°ë‚˜ í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ëŠ” ê²½ìš° (LlamaParse ë“± í•„ìš”) -> OCR í•„ìš”
            - 'fast': í…ìŠ¤íŠ¸ ìœ„ì£¼ì˜ ì¼ë°˜ì ì¸ ë¬¸ì„œ (PyMuPDF4LLM ì‚¬ìš©)
            - 'precise': í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ê°€ ì„ì—¬ìˆì–´ ì •ë°€í•œ ë ˆì´ì•„ì›ƒ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°
        """
        doc = fitz.open(file_path)
        total_pages = len(doc)

        # 1. ë„ˆë¬´ í° íŒŒì¼ ì˜ˆì™¸ ì²˜ë¦¬ (ì˜ˆ: 500í˜ì´ì§€ ì´ìƒì€ ì¼ë‹¨ ê²½ê³ )
        if total_pages > 500:
            print(f"[Warn] Large file detected: {total_pages} pages.")

        # 2. ìƒ˜í”Œë§ í˜ì´ì§€ ì¸ë±ìŠ¤ ì„ ì •
        sample_indices = set()

        # ì• 3í˜ì´ì§€
        for i in range(min(3, total_pages)):
            sample_indices.add(i)

        # ì¤‘ê°„ 1í˜ì´ì§€
        if total_pages > 3:
            sample_indices.add(total_pages // 2)

        # ë’¤ 2í˜ì´ì§€
        if total_pages > 1:
            sample_indices.add(total_pages - 1)
        if total_pages > 2:
            sample_indices.add(total_pages - 2)

        sorted_indices = sorted(list(sample_indices))

        # 3. ìƒ˜í”Œë§ ë¶„ì„
        image_count = 0
        text_length = 0
        page_count = 0

        for idx in sorted_indices:
            if idx >= total_pages:
                continue

            page = doc[idx]
            page_count += 1

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = page.get_text()
            text_length += len(text.strip())

            # ì´ë¯¸ì§€ ê°ì²´ ì¹´ìš´íŠ¸
            images = page.get_images(full=True)
            image_count += len(images)

        doc.close()

        # 4. ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ì „ëµ ê²°ì •

        # í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´ (í˜ì´ì§€ë‹¹)
        avg_text_per_page = text_length / page_count if page_count > 0 else 0

        # í‰ê·  ì´ë¯¸ì§€ ìˆ˜ (í˜ì´ì§€ë‹¹)
        avg_images_per_page = image_count / page_count if page_count > 0 else 0

        print(
            f"[PDF Analysis] Avg Text: {avg_text_per_page:.1f}, Avg Images: {avg_images_per_page:.1f}"
        )

        # Case A: í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ìŒ (OCR í•„ìš”)
        if avg_text_per_page < 50:
            return ParsingStrategy.IMAGE

        # Case B: ì´ë¯¸ì§€ê°€ ë§ê³  í…ìŠ¤íŠ¸ë„ ì–´ëŠì •ë„ ìˆìŒ (ë³µì¡í•œ ë ˆì´ì•„ì›ƒ ê°€ëŠ¥ì„±)
        elif avg_images_per_page > 2:
            return ParsingStrategy.MIXED

        # Case C: í…ìŠ¤íŠ¸ ìœ„ì£¼
        else:
            return ParsingStrategy.TEXT

    def _parse_with_pymupdf(self, file_path: str) -> list[dict]:
        """ê¸°ì¡´ PyMuPDF4LLM ê¸°ë°˜ íŒŒì‹± ë¡œì§"""
        md_text_chunks = pymupdf4llm.to_markdown(file_path, page_chunks=True)

        results = []
        for chunk in md_text_chunks:
            results.append(
                {
                    "text": chunk["text"],
                    "page": chunk["metadata"]["page"] + 1,
                }
            )
        return results

    def _get_cache_path(self, file_path: str) -> str:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„± (ì›ë³¸íŒŒì¼_parsed.json)"""
        return f"{file_path}_parsed.json"

    def _save_cache(self, file_path: str, data: list[dict]):
        """íŒŒì‹± ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        try:
            cache_path = self._get_cache_path(file_path)
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ [Cache Saved] {cache_path}")
        except Exception as e:
            print(f"âš ï¸ Failed to save cache: {e}")

    def _load_cache(self, file_path: str) -> list[dict]:
        """ìºì‹œëœ íŒŒì‹± ê²°ê³¼ ë¡œë“œ"""
        cache_path = self._get_cache_path(file_path)
        if os.path.exists(cache_path):
            try:
                with open(cache_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                print(f"â™»ï¸ [Cache Hit] Loaded parsing result from {cache_path}")
                return data
            except Exception as e:
                print(f"âš ï¸ Cache load failed: {e}")
        return None

    def _get_llamaparse_api_key(self) -> Optional[str]:
        """
        LlamaParse API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        1) í™˜ê²½ë³€ìˆ˜ ìš°ì„ 
        2) í˜„ì¬ ì‚¬ìš©ì(user_id)ì˜ LlamaParse ìê²©ì¦ëª…
        3) ì‚¬ìš©ì ë¯¸ì§€ì • ì‹œ ì‹œìŠ¤í…œ ë‚´ ì²« ë²ˆì§¸ ìœ íš¨ ìê²©ì¦ëª…
        """
        env_key = os.getenv("LLAMA_CLOUD_API_KEY")
        if env_key:
            return env_key

        provider = (
            self.db.query(LLMProvider).filter(LLMProvider.name == "llamaparse").first()
        )
        if not provider:
            return None

        query = self.db.query(LLMCredential).filter(
            LLMCredential.provider_id == provider.id,
            LLMCredential.is_valid == True,
        )
        if self.user_id:
            query = query.filter(LLMCredential.user_id == self.user_id)

        cred = query.order_by(LLMCredential.created_at.desc()).first()

        # ì‚¬ìš©ìë³„ í‚¤ê°€ ì—†ìœ¼ë©´ ì‹œìŠ¤í…œ ë‚´ ì•„ë¬´ ìœ íš¨ í‚¤ë‚˜ ì‚¬ìš© (í™˜ê²½ë³€ìˆ˜ ì ‘ê·¼ê³¼ ë™ì¼ ë ˆë²¨)
        if not cred:
            cred = (
                self.db.query(LLMCredential)
                .filter(
                    LLMCredential.provider_id == provider.id,
                    LLMCredential.is_valid == True,
                )
                .order_by(LLMCredential.created_at.desc())
                .first()
            )

        if not cred:
            return None

        try:
            cfg = json.loads(cred.encrypted_config)
            return cfg.get("apiKey")
        except Exception as e:
            print(f"[Warning] Failed to parse LlamaParse credential: {e}")
            return None

    def _parse_with_llamaparse(self, file_path: str) -> list[dict]:
        """LlamaParse API ì—°ë™ (ìºì‹± ì ìš©)"""

        # 1. ìºì‹œ í™•ì¸
        cached_data = self._load_cache(file_path)
        if cached_data:
            return cached_data

        # ë¹„ìš© ì˜ˆì¸¡ ë¡œê·¸ ì¶œë ¥
        est = self._estimate_llamaparse_cost(file_path)
        print(
            f"ğŸ’° [ë¹„ìš© ì˜ˆì¸¡] í˜ì´ì§€ ìˆ˜: {est['pages']}, í¬ë ˆë”§: {est['credits']}, ë¹„ìš©: ${est['cost_usd']:.4f}"
        )

        try:
            from llama_parse import LlamaParse
        except ImportError:
            print(
                "âŒ LlamaParse ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'pip install llama-parse'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
            )
            return []

        api_key = self._get_llamaparse_api_key()
        if not api_key:
            print(
                "âŒ LlamaParse API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                "í™˜ê²½ë³€ìˆ˜ LLAMA_CLOUD_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ì„¤ì • > LLM Providerì—ì„œ í‚¤ë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”."
            )
            return []

        print("ğŸš€ LlamaParse í´ë¼ìš°ë“œ ì²˜ë¦¬ ì‹œì‘...")

        try:
            # íŒŒì„œ ì´ˆê¸°í™”
            # result_type="markdown"ì´ ê¸°ë³¸ê°’ì´ì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
            # language="ko"ë¥¼ ì„¤ì •í•˜ì—¬ í•œêµ­ì–´ ì¸ì‹ë¥  í–¥ìƒ
            parser = LlamaParse(
                api_key=api_key,
                result_type="markdown",
                verbose=True,
                language="ko",
                fast_mode=True,
            )

            # JSON ê²°ê³¼ë¥¼ ë°›ì•„ì•¼ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ í™•ì‹¤í•˜ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆìŒ
            # get_json_resultëŠ” íŒŒì¼ë‹¹ í•˜ë‚˜ì˜ ê²°ê³¼ ê°ì²´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•¨
            json_results = parser.get_json_result(file_path)

            # [Debug] êµ¬ì¡° í™•ì¸
            # print(f"ğŸ” [LlamaParse Raw Result]: {json_results}")

            parsed_results = []
            full_text_for_debug = ""

            if json_results and isinstance(json_results, list):
                first_result = json_results[0]
                # 'pages' í‚¤ì— ê° í˜ì´ì§€ë³„ íŒŒì‹± ê²°ê³¼ê°€ ë‹´ê²¨ìˆìŒ
                pages = first_result.get("pages", [])

                for p in pages:
                    # 'md' í‚¤ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ í‚¤ í™•ì¸
                    # 'md' í‚¤ê°€ ì—†ìœ¼ë©´ 'text' í‚¤ë¥¼ ì‚¬ìš© (fast_mode ë“±ì—ì„œ ë°œìƒ)
                    md_text = p.get("md") or p.get("text") or ""
                    parsed_results.append(
                        {
                            "text": md_text,  # ë§ˆí¬ë‹¤ìš´ ë³€í™˜ í…ìŠ¤íŠ¸
                            "page": p["page"],  # í˜ì´ì§€ ë²ˆí˜¸
                        }
                    )
                    full_text_for_debug += f"\n--- Page {p['page']} ---\n{md_text}\n"

            # [Debug] íŒŒì‹±ëœ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            # try:
            #     base_dir = os.path.dirname(file_path)
            #     file_name = os.path.basename(file_path)
            #     debug_file_name = f"{os.path.splitext(file_name)[0]}_parsed.md"
            #     debug_file_path = os.path.join(base_dir, debug_file_name)

            #     with open(debug_file_path, "w", encoding="utf-8") as f:
            #         f.write(full_text_for_debug)
            #     print(f"ğŸ’¾ [Debug] Parsed content saved to: {debug_file_path}")
            # except Exception as e:
            #     print(f"âš ï¸ Failed to save debug file: {e}")

            print(f"LlamaParse ì™„ë£Œ: ì´ {len(parsed_results)} í˜ì´ì§€ ë³€í™˜ë¨.")

            # 2. ê²°ê³¼ ìºì‹±
            self._save_cache(file_path, parsed_results)

            return parsed_results

        except Exception as e:
            print(f"LlamaParse ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []

    def _estimate_llamaparse_cost(self, file_path: str) -> dict:
        """
        LlamaParse ì˜ˆì¸¡ ë¹„ìš© ê³„ì‚°
        ê¸°ì¤€: Standard Mode (3 credits/page), $1 = 1000 credits
        """
        try:
            # PDFê°€ ì•„ë‹Œ ê²½ìš° fitz.open()ì´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜ˆì™¸ ì²˜ë¦¬
            ext = os.path.splitext(file_path)[1].lower()
            if ext not in [".pdf", ".xps", ".epub", ".mobi", ".fb2", ".cbz", ".svg"]:
                # ì´ë¯¸ì§€ íŒŒì¼(png, jpg ë“±)ì€ fitzë¡œ ì—´ ìˆ˜ ìˆì§€ë§Œ, ì—‘ì…€/ì›Œë“œëŠ” ë¶ˆê°€
                # ì¼ë‹¨ 0ìœ¼ë¡œ ë¦¬í„´í•˜ì—¬ fallback ìœ ë„
                if ext not in [".png", ".jpg", ".jpeg", ".tiff", ".bmp"]:
                    return {"pages": 0, "credits": 0, "cost_usd": 0.0}

            doc = fitz.open(file_path)
            total_pages = len(doc)
            doc.close()

            # Standard Mode ê¸°ì¤€ (í˜ì´ì§€ë‹¹ 3 í¬ë ˆë”§)
            credits_per_page = 3
            total_credits = total_pages * credits_per_page
            cost_usd = total_credits / 1000.0

            return {
                "pages": total_pages,
                "credits": total_credits,
                "cost_usd": cost_usd,
            }
        except Exception as e:
            print(f"[Warning] Cost estimation failed: {e}")
            return {"pages": 0, "credits": 0, "cost_usd": 0.0}

    def _parse_excel_csv(self, file_path: str) -> list[dict]:
        """Excel/CSV íŒŒì¼ íŒŒì‹± (ëª¨ë“  ì‹œíŠ¸ ì²˜ë¦¬)"""
        text_content = ""
        ext = os.path.splitext(file_path)[1].lower()

        try:
            if ext == ".csv":
                df = pd.read_csv(file_path)
                text_content += f"# CSV Content\n\n{df.to_markdown(index=False)}\n"
            else:
                # sheet_name=None -> ëª¨ë“  ì‹œíŠ¸ë¥¼ dictë¡œ ë°˜í™˜
                xls = pd.read_excel(file_path, sheet_name=None)
                for sheet_name, df in xls.items():
                    text_content += f"\n# Sheet: {sheet_name}\n\n"
                    text_content += df.to_markdown(index=False) + "\n"

            # ì—‘ì…€ì€ í˜ì´ì§€ ê°œë…ì´ ëª¨í˜¸í•˜ë¯€ë¡œ ì „ì²´ë¥¼ 1í˜ì´ì§€ë¡œ ì·¨ê¸‰í•˜ê±°ë‚˜ ì ì ˆíˆ ë¶„í• 
            return [{"text": text_content, "page": 1}]
        except Exception as e:
            print(f"Excel/CSV parsing failed: {e}")
            return []

    def _parse_docx(self, file_path: str) -> list[dict]:
        """Word(.docx) íŒŒì¼ íŒŒì‹±"""
        try:
            doc = DocxDocument(file_path)
            full_text = []
            for para in doc.paragraphs:
                if para.text.strip():
                    full_text.append(para.text)

            # ê°„ë‹¨í•œ í‘œ ì²˜ë¦¬
            for table in doc.tables:
                for row in table.rows:
                    row_text = [cell.text for cell in row.cells]
                    full_text.append(" | ".join(row_text))

            return [{"text": "\n".join(full_text), "page": 1}]
        except Exception as e:
            print(f"Docx parsing failed: {e}")
            return []

    def _parse_txt(self, file_path: str) -> list[dict]:
        """Text/Markdown íŒŒì¼ íŒŒì‹±"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
            return [{"text": content, "page": 1}]
        except Exception as e:
            print(f"Text parsing failed: {e}")
            return []

    def _is_mixed_quality_poor(self, results: list[dict]) -> bool:
        """MIXED ëª¨ë“œ í’ˆì§ˆ ê²€ì‚¬: ë ˆì´ì•„ì›ƒì´ ì‹¬ê°í•˜ê²Œ ê¹¨ì¡ŒëŠ”ì§€ í™•ì¸"""
        total_text = "".join([r["text"] for r in results])

        # íœ´ë¦¬ìŠ¤í‹± 1: ì•Œ ìˆ˜ ì—†ëŠ” íŠ¹ìˆ˜ë¬¸ìë‚˜ ê³µë°± íŒ¨í„´ì´ ë„ˆë¬´ ë§ì€ ê²½ìš°
        if len(total_text) > 0:
            broken_char_count = total_text.count("\ufffd")
            if (broken_char_count / len(total_text)) > 0.05:  # 5% ì´ìƒ ê¹¨ì§
                print("Reason: High broken character rate in MIXED mode.")
                return True

        # íœ´ë¦¬ìŠ¤í‹± 2: ë§ˆí¬ë‹¤ìš´ êµ¬ì¡°ê°€ ê±°ì˜ ì—†ìŒ (í—¤ë” #ì´ ë„ˆë¬´ ì ìŒ)
        # ì¼ë°˜ì ì¸ ë¬¸ì„œë¼ë©´ í˜ì´ì§€ë‹¹ ì ì–´ë„ 1~2ê°œì˜ í—¤ë”ëŠ” ìˆì–´ì•¼ í•¨
        page_count = len(results)
        header_count = total_text.count("\n#")
        if (
            page_count > 0 and (header_count / page_count) < 0.2
        ):  # 5í˜ì´ì§€ë‹¹ í—¤ë” 1ê°œ ë¯¸ë§Œ
            print("Reason: Too few markdown headers found.")
            return True

        return False

    def _is_text_quality_poor(self, file_path: str, results: list[dict]) -> bool:
        """TEXT ëª¨ë“œ í’ˆì§ˆ ê²€ì‚¬"""
        total_text = "".join([r["text"] for r in results])

        # 1. ê¸€ì ìˆ˜ê°€ ë„ˆë¬´ ì ìŒ (50ì ë¯¸ë§Œ)
        if len(total_text.strip()) < 50:
            print("Reason: Too few characters extracted.")
            return True

        # 2. ê¹¨ì§„ ë¬¸ì(replacement character ) ë¹„ìœ¨ í™•ì¸
        broken_char_count = total_text.count("\ufffd")  # or other garbage chars
        if len(total_text) > 0 and (
            broken_char_count / len(total_text) > 0.05
        ):  # 5% ì´ìƒ
            print("Reason: Too many broken characters.")
            return True

        # 3. (ê³ ê¸‰) PyMuPDFë¡œ í‘œ(Table)ëŠ” ê°ì§€ë˜ëŠ”ë°, ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì—ëŠ” ë§ˆí¬ë‹¤ìš´ í‘œ ë¬¸ë²•(|---|)ì´ ì—†ëŠ” ê²½ìš°
        try:
            doc = fitz.open(file_path)
            has_table_but_no_markdown = False

            # ì„±ëŠ¥ì„ ìœ„í•´ ì•ë¶€ë¶„ 5í˜ì´ì§€ë§Œ ê²€ì‚¬
            for i in range(min(5, len(doc))):
                page = doc[i]
                tables = page.find_tables()
                if tables and len(tables.tables) > 0:
                    # í•´ë‹¹ í˜ì´ì§€ì˜ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì°¾ê¸°
                    page_text = results[i]["text"] if i < len(results) else ""
                    # í‘œëŠ” ìˆëŠ”ë° ë§ˆí¬ë‹¤ìš´ í‘œ êµ¬ë¬¸('|')ì´ ì „í˜€ ì—†ë‹¤ë©´ íŒŒì‹± ì‹¤íŒ¨ë¡œ ê°„ì£¼
                    if "|" not in page_text:
                        print(
                            f"Reason: Table detected on page {i + 1} but no markdown table found."
                        )
                        has_table_but_no_markdown = True
                        break

            doc.close()
            if has_table_but_no_markdown:
                return True

        except Exception as e:
            print(f"[Warning] Table check failed: {e}")
            # ì—ëŸ¬ ë‚˜ë©´ ì•ˆì „í•˜ê²Œ False ë°˜í™˜ (Flow ì¤‘ë‹¨ ì•ˆ í•¨)
            return False

        return False

    def _request_llamaparse_approval(
        self, file_path: str, document_id: UUID
    ) -> list[dict]:
        """
        LlamaParse í˜¸ì¶œ ì „ ë¹„ìš© ê³„ì‚° í›„ 'ìŠ¹ì¸ ëŒ€ê¸°' ìƒíƒœë¡œ ë³€ê²½í•˜ê³  ì¤‘ë‹¨í•¨.
        """
        if not document_id:
            # document_idê°€ ì—†ìœ¼ë©´(ë””ë²„ê·¸/í…ŒìŠ¤íŠ¸ ëª¨ë“œ) ê·¸ëƒ¥ ì§„í–‰
            print("No document_id provided. Skipping approval and running LlamaParse.")
            return self._parse_with_llamaparse(file_path)

        # 1. ë¹„ìš© ê³„ì‚°
        est = self._estimate_llamaparse_cost(file_path)

        # 2. DB ì—…ë°ì´íŠ¸ (ìƒíƒœ: waiting_for_approval)
        doc = self.db.query(Document).get(document_id)
        if doc:
            doc.status = "waiting_for_approval"
            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ì— ë¹„ìš© ì •ë³´ ë³‘í•©
            new_meta = dict(doc.meta_info or {})
            new_meta.update({"cost_estimate": est, "strategy": "llamaparse_fallback"})
            doc.meta_info = new_meta
            self.db.commit()

        print(
            f"â¸ï¸ [Approval Required] Document {document_id} paused for LlamaParse cost approval."
        )

        # 3. ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜í•˜ì—¬ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨
        return []

    def _parse_pdf(self, file_path: str, document_id: UUID = None) -> list[dict]:
        """
        PDF íŒŒì‹± ë©”ì¸ ì§„ì…ì .
        ì ì ˆí•œ íŒŒì„œ(PyMuPDF / LlamaParse)ë¥¼ ì„ íƒí•˜ê³ ,
        í’ˆì§ˆ ì €í•˜ ì‹œ Fallback ë¡œì§ì„ ìˆ˜í–‰ (ë¹„ìš© ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ í¬í•¨)
        """
        # 1. íŒŒì¼ ì„±ê²© íŒŒì•…
        parsing_strategy = self._analyze_pdf_type(file_path)
        print(f"[{file_path}] Parsing Strategy: {parsing_strategy.value}")

        # Case 1: ì´ë¯¸ì§€ ìœ„ì£¼ (OCR í•„ìˆ˜) -> ìŠ¹ì¸ ìš”ì²­
        if parsing_strategy == ParsingStrategy.IMAGE:
            print("Strategy is IMAGE. Requesting approval for LlamaParse.")
            return self._request_llamaparse_approval(file_path, document_id)

        # Case 2: í˜¼í•©í˜• (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)
        elif parsing_strategy == ParsingStrategy.MIXED:
            # 1ì°¨ ì‹œë„: PyMuPDF (ë¹ ë¦„)
            results = self._parse_with_pymupdf(file_path)

            # í’ˆì§ˆ ê²€ì‚¬: ê²°ê³¼ë¬¼ì´ 'ë‚œì¡'í•œì§€ í™•ì¸
            if self._is_mixed_quality_poor(results):
                print(
                    "Mixed parsing quality is poor. Requesting approval for LlamaParse."
                )
                return self._request_llamaparse_approval(file_path, document_id)

            return results

        # Case 3: í…ìŠ¤íŠ¸ ìœ„ì£¼
        else:  # ParsingStrategy.TEXT
            # 1ì°¨ ì‹œë„: PyMuPDF
            results = self._parse_with_pymupdf(file_path)

            # í’ˆì§ˆ ê²€ì‚¬: í…ìŠ¤íŠ¸ ëˆ„ë½, ê¹¨ì§, í‘œ êµ¬ì¡° ì´ìƒ í™•ì¸
            if self._is_text_quality_poor(file_path, results):
                print(
                    "Text parsing quality is poor. Requesting approval for LlamaParse."
                )
                return self._request_llamaparse_approval(file_path, document_id)

            return results

    def _create_chunks(self, text_blocks: list[dict]) -> list[dict]:
        """
        íŒŒì‹±ëœ í…ìŠ¤íŠ¸ë¥¼ ë” ì‘ì€ ì¡°ê°(Chunk)ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
        """
        final_chunks = []

        for block in text_blocks:
            splits = self.text_splitter.split_text(block["text"])
            for split in splits:
                final_chunks.append(
                    {"content": split, "metadata": {"page": block["page"]}}
                )
        return final_chunks

    def _save_chunks_to_pgvector(
        self, document_id: UUID, knowledge_base_id: UUID, chunks: list[dict]
    ):
        """
        í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ OpenAIì— ë³´ë‚´ì„œ 'ì˜ë¯¸ ë²¡í„°'ë¡œ ë°”ê¾¼ ë’¤, DocumentChunk í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤.
        ê¸°ì¡´ ì²­í¬ê°€ ìˆë‹¤ë©´ ì‚­ì œí•˜ê³  ìƒˆë¡œ ì €ì¥í•©ë‹ˆë‹¤ (Clean & Insert).
        """
        print(f"ğŸ” [Debug] _save_chunks_to_pgvector ì‹œì‘: doc_id={document_id}")
        # 0. ê¸°ì¡´ ì²­í¬ ì‚­ì œ (Clean Step)
        try:
            del_count = (
                self.db.query(DocumentChunk)
                .filter(DocumentChunk.document_id == document_id)
                .delete()
            )
            self.db.commit()
            print(f"ğŸ—‘ï¸ [Debug] ê¸°ì¡´ ì²­í¬ {del_count}ê°œ ì‚­ì œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ [Debug] ê¸°ì¡´ ì²­í¬ ì‚­ì œ ì¤‘ ì—ëŸ¬: {e}")

        # TODO: í† í° ê³„ì‚°ì„ ìœ„í•œ ì¸ì½”ë” ì„¤ì •
        try:
            encoding = tiktoken.encoding_for_model(self.ai_model)
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base")  # gpt-4ë¡œ ê°€ì •í•˜ê³  ê³„ì‚°

        # DBì—ì„œ API Key ê°€ì ¸ì˜¤ê¸° (í™˜ê²½ë³€ìˆ˜ ì˜ì¡´ ì œê±°)
        from db.models.llm import LLMProvider

        api_key = None

        doc = self.db.query(Document).filter(Document.id == document_id).first()
        if not doc:
            print("âŒ [Debug] ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            raise ValueError("ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        user_id = doc.knowledge_base.user_id
        print(f"ğŸ” [Debug] ë¬¸ì„œ ì†Œìœ ì ID: {user_id}")

        user_crd = (
            self.db.query(LLMCredential)
            .join(LLMProvider)
            .filter(
                LLMCredential.user_id == user_id,
                LLMCredential.is_valid,
                LLMProvider.name == "openai",
            )
            .first()
        )

        if user_crd:
            print(f"âœ… [Debug] OpenAI ìê²© ì¦ëª… ë°œê²¬ (ID: {user_crd.id})")
            try:
                config = json.loads(user_crd.encrypted_config)
                api_key = config.get("apiKey")
            except Exception as e:
                print(f"[Debug] Credential config íŒŒì‹± ì‹¤íŒ¨: {e}")

        if not api_key:
            raise ValueError(
                "ì‚¬ìš©ìì˜ OpenAI API Keyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë“±ë¡í•´ì£¼ì„¸ìš”."
            )
            print("âš ï¸ [Debug] OpenAI ìê²© ì¦ëª…ì„ ì°¾ì§€ ëª»í•¨")
        print(f"âœ… [Debug] API Key í™•ë³´ ì™„ë£Œ (Key: {api_key[:8]}...)")

        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (API Key ëª…ì‹œ)
        embeddings_model = OpenAIEmbeddings(model=self.ai_model, openai_api_key=api_key)

        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = [chunk["content"] for chunk in chunks]
        print(f"ğŸ” [Debug] ì„ë² ë”© ìš”ì²­ ì‹œì‘ (ì²­í¬ ê°œìˆ˜: {len(texts)}ê°œ)")

        # 2. ì„ë² ë”© ìƒì„± (ì¼ê´„ í˜¸ì¶œ) - ì‹¤ì œ API ì‚¬ìš©!
        try:
            embedded_vectors = embeddings_model.embed_documents(texts)
            print("âœ… [Debug] ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ [Debug] OpenAI Embedding Error: {e}")
            raise e

        # 3. DB ê°ì²´ ìƒì„±
        try:
            chunk_objects = []
            for i, chunk in enumerate(chunks):
                content = chunk["content"]
                token_count = len(encoding.encode(content))

                db_chunk = DocumentChunk(
                    document_id=document_id,
                    knowledge_base_id=knowledge_base_id,  # ê²€ìƒ‰ ìµœì í™”ìš©
                    content=content,
                    embedding=embedded_vectors[i],
                    chunk_index=i,
                    token_count=token_count,
                    metadata_=chunk["metadata"],
                )
                chunk_objects.append(db_chunk)

            print(
                f"ğŸ“¦ [Debug] ì €ì¥í•  ê°ì²´ {len(chunk_objects)}ê°œ ìƒì„±ë¨. DBì— ì¶”ê°€(add) ì‹œë„..."
            )
            self.db.add_all(chunk_objects)
            print("ğŸ’¾ [Debug] ì»¤ë°‹(Commit) ì‹œë„...")
            self.db.commit()
            print("ğŸ‰ [Debug] DB ì €ì¥ ë° ì»¤ë°‹ ì„±ê³µ!")

        except Exception as e:
            print(f"âŒ [Debug] DB ì €ì¥ ì‹¤íŒ¨ (Commit Error): {e}")
            self.db.rollback()  # ë¡¤ë°± ì‹œë„
            raise e

    def _create_chunks(self, text_blocks: list[dict]) -> list[dict]:
        """
        í…ìŠ¤íŠ¸ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„, ì„¤ì •ëœ chunk_sizeì™€ chunk_overlapì— ë”°ë¼ ì²­í‚¹í•©ë‹ˆë‹¤.
        ê° ì²­í¬ëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ ë¸”ë¡ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ë³‘í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        chunks = []
        for block in text_blocks:
            text = block["text"]
            metadata = block.get("metadata", {})

            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µí•  ìˆ˜ë„ ìˆìŒ (ì„ íƒì‚¬í•­)
            if not text.strip():
                continue

            splits = self.text_splitter.split_text(text)

            for split in splits:
                chunks.append(
                    {
                        "content": split,
                        "metadata": metadata,  # í˜ì´ì§€ ë²ˆí˜¸ ë“± ì›ë³¸ ë©”íƒ€ë°ì´í„° ë³´ì¡´
                    }
                )

        return chunks

    def _finalize_ingestion(
        self, document_id: UUID, knowledge_base_id: UUID, text_blocks: list[dict]
    ):
        """
        í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ë°›ì•„ ì²­í‚¹ -> ì„ë² ë”© -> ì €ì¥ -> ì™„ë£Œ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        # 2ë‹¨ê³„: ì²­í‚¹
        self._update_progress(
            document_id, 50, "AIê°€ ì½ê¸° ì¢‹ê²Œ ë¬¸ì„œë¥¼ ì¡°ê°ë‚´ê³  ìˆìŠµë‹ˆë‹¤..."
        )
        chunks = self._create_chunks(text_blocks)

        # 3 & 4ë‹¨ê³„: ì„ë² ë”© ë° ì €ì¥
        self._update_progress(
            document_id, 70, "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•  ì¤€ë¹„ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
        )
        self._save_chunks_to_pgvector(document_id, knowledge_base_id, chunks)

        # ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        self._update_progress(document_id, 100, "ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        self._update_status(document_id, "completed")

    def _update_status(self, document_id: UUID, status: str, error_message: str = None):
        doc = self.db.query(Document).get(document_id)
        if doc:
            doc.status = status
            if error_message:
                doc.error_message = error_message
            self.db.commit()

    def _update_progress(self, document_id: UUID, progress: int, message: str):
        """
        ë¬¸ì„œ ì²˜ë¦¬ ì§„í–‰ë¥ (%)ê³¼ í˜„ì¬ ë‹¨ê³„ ë©”ì‹œì§€ë¥¼ meta_infoì— ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        """
        doc = self.db.query(Document).get(document_id)
        if doc:
            new_meta = dict(doc.meta_info or {})
            new_meta.update(
                {"processing_progress": progress, "processing_current_step": message}
            )
            doc.meta_info = new_meta
            self.db.commit()

    async def analyze_document(self, document_id: UUID) -> dict:
        """
        ë¬¸ì„œ ë¶„ì„: í˜ì´ì§€ ìˆ˜, ë¹„ìš© ì˜ˆì¸¡ ë“±ì„ ë°˜í™˜
        """
        doc = self.db.query(Document).get(document_id)
        if not doc:
            raise ValueError("Document not found")

        # 1. ë¹„ìš© ì˜ˆì¸¡ (FileDataSource ì‚¬ìš©)
        try:
            # ì„ì‹œë¡œ FILE íƒ€ì… ê°€ì • (API ë“±ì€ 0 ë°˜í™˜)
            data_source = self._get_data_source(doc.source_type)
            source_config = {}
            if doc.source_type == SourceType.FILE:
                source_config = {"file_path": doc.file_path}

            cost_info = data_source.estimate_cost(source_config)
        except Exception as e:
            print(f"Cost estimation failed: {e}")
            cost_info = {"pages": 0, "credits": 0, "cost_usd": 0.0}

        # 2. íŒŒì¼ íƒ€ì… ë¶„ì„ (ì„ íƒ ì‚¬í•­)
        # parsing_strategy = self._analyze_pdf_type(doc.file_path)

        # 3. ìºì‹œ í™•ì¸ (íŒŒì¼ì¸ ê²½ìš°ì—ë§Œ)
        is_cached = False
        cache_path = ""

        if doc.source_type == SourceType.FILE and doc.file_path:
            cache_path = self._get_cache_path(doc.file_path)
            is_cached = os.path.exists(cache_path)

        print(
            f"ğŸ” [Debug] analyze_document: filename={doc.filename}, is_cached={is_cached}, path={cache_path}"
        )

        return {
            "cost_estimate": cost_info,
            "filename": doc.filename,
            "is_cached": is_cached,
            # "recommended_strategy": parsing_strategy
        }

    def _get_cache_path(self, file_path: str) -> str:
        """
        LlamaParse ê²°ê³¼ ìºì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        (ì˜ˆ: uploads/file.pdf -> uploads/file.pdf.md)
        """
        if not file_path:
            return ""
        return f"{file_path}.md"

    def preview_chunking(
        self,
        file_path: str,
        chunk_size: int,
        chunk_overlap: int,
        segment_identifier: str,
        remove_urls_emails: bool = False,
        remove_whitespace: bool = True,
        strategy: str = "general",  # "general" or "llamaparse",
        source_type: SourceType = SourceType.FILE,
        meta_info: dict = None,
    ) -> list[dict]:
        """
        DB ì €ì¥ ì—†ì´ ë©”ëª¨ë¦¬ ìƒì—ì„œ ì²­í‚¹ ê²°ê³¼ë¥¼ ë¯¸ë¦¬ë´…ë‹ˆë‹¤.
        strategyì— ë”°ë¼ ì¼ë°˜ íŒŒì‹± ë˜ëŠ” ì •ë°€ íŒŒì‹±(LlamaParse)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        try:
            if source_type == SourceType.API:
                # API ë°˜í™˜ê°’ ì²˜ë¦¬, í—¤ë” ë³µí˜¸í™”
                api_config = meta_info.get("api_config", {})
                headers = api_config.get("headers")
                if headers and isinstance(headers, str):
                    try:
                        from core.security import security_service

                        decrypted_json = security_service.decrypt(headers)
                        api_config["headers"] = json.loads(decrypted_json)
                    except Exception as e:
                        print(f"Failed to decrypt headers: {e}")
                        api_config["headers"] = {}

                data_source = ApiDataSource()
                source_config = api_config
            else:
                data_source = FileDataSource()
                source_config = {
                    "file_path": file_path,
                    "strategy": strategy,  # "general" or "llamaparse"
                }

            text_blocks = data_source.fetch_text(source_config)
            full_text = "\n".join([block["text"] for block in text_blocks])
        except Exception as e:
            print(f"Preview parsing failed: {e}")
            return []

        # 2. ì „ì²˜ë¦¬
        if remove_urls_emails:
            # URL ì œê±°
            full_text = re.sub(
                r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+",
                "",
                full_text,
            )
            # ì´ë©”ì¼ ì œê±°
            full_text = re.sub(r"[\w\.-]+@[\w\.-]+", "", full_text)

        if remove_whitespace:
            # ì—°ì†ëœ ê³µë°±, íƒ­ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
            full_text = re.sub(r"[ \t]+", " ", full_text)
            # ì—°ì†ëœ ì¤„ë°”ê¿ˆì´ 3ê°œ ì´ìƒì´ë©´ 2ê°œ(\n\n)ë¡œ ì¶•ì†Œ (ë¬¸ë‹¨ êµ¬ë¶„ ìœ ì§€)
            full_text = re.sub(r"\n{3,}", "\n\n", full_text)

        # 3. ì²­í‚¹ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
        # segment_identifierê°€ ìœ íš¨í•˜ë©´ separator ëª©ë¡ì˜ ìµœìš°ì„  ìˆœìœ„ë¡œ ì¶”ê°€
        separators = ["\n\n", "\n", ".", " ", ""]
        if segment_identifier and segment_identifier not in separators:
            # íŠ¹ìˆ˜ ë¬¸ì(escaped) ì²˜ë¦¬ í•„ìš”í•  ìˆ˜ ìˆìŒ. ì¼ë‹¨ ìˆëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©.
            # ì‚¬ìš©ìê°€ "\n\n"ì„ ì…ë ¥í•˜ë©´ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ë“¤ì–´ì˜¤ë¯€ë¡œ, ì‹¤ì œ ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë¡œì§ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ.
            # í”„ë¡ íŠ¸ì—ì„œ ì‹¤ì œ ì¤„ë°”ê¿ˆì„ ë³´ë‚´ê±°ë‚˜, ì—¬ê¸°ì„œ ë³€í™˜í•´ì•¼ í•¨.
            # ì¼ë‹¨ì€ ë‹¨ìˆœ ë¬¸ìì—´ ë§¤ì¹­ìœ¼ë¡œ ê°€ì •í•˜ë˜, \nì€ íŠ¹ë³„ ì·¨ê¸‰
            processed_identifier = segment_identifier.replace("\\n", "\n")
            if processed_identifier not in separators:
                separators.insert(0, processed_identifier)

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=separators,
            keep_separator=True,
        )

        splits = splitter.split_text(full_text)

        # 4. ê²°ê³¼ í¬ë§·íŒ… & í† í° ê³„ì‚°
        try:
            encoding = tiktoken.encoding_for_model(self.ai_model)
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base")

        preview_segments = []
        for split in splits:
            token_count = len(encoding.encode(split))
            preview_segments.append(
                {
                    "content": split,
                    "token_count": token_count,
                    "char_count": len(split),
                }
            )

        return preview_segments
