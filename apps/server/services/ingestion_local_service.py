import hashlib
import json
import os  # í´ë” ë§Œë“¤ê¸°ìš©
import re
import shutil  # íŒŒì¼ ë³µì‚¬ìš©
from enum import Enum
from uuid import UUID

import tiktoken
from fastapi import UploadFile
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from sqlalchemy.orm import Session

from db.models.knowledge import Document, DocumentChunk, SourceType
from db.models.llm import LLMCredential
from services.data_sources import ApiDataSource, BaseDataSource, FileDataSource


class ParsingStrategy(str, Enum):
    TEXT = "text"
    MIXED = "mixed"
    IMAGE = "image"


class IngestionService:
    def __init__(
        self,
        db: Session,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        ai_model: str = "text-embedding-3-small",
    ):
        self.db = db
        self.ai_model = ai_model

        # ì²­í‚¹ ì „ëµ ì„¤ì •
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            # ë¬¸ë‹¨ ë°”ë€” ë•Œ, ì¤„ ë°”ê¿€ ë•Œ, ë§ˆì¹¨í‘œ, ë„ì–´ì“°ê¸°ì¼ ë•Œ ìë¥¸ë‹¤
            separators=["\n\n", "\n", ".", " ", ""],
            keep_separator=True,
        )

    def _get_data_source(self, source_type: str) -> BaseDataSource:
        if source_type == SourceType.FILE:
            return FileDataSource()
        elif source_type == SourceType.API:
            return ApiDataSource()
        # Default fallback for legacy data or if source_type is string "FILE"
        if str(source_type) == "FILE":
            return FileDataSource()
        raise ValueError(f"Unknown source type: {source_type}")

    def save_temp_file(self, file: UploadFile) -> str:
        """
        ì„¤ëª…: ë©”ëª¨ë¦¬ì— ìˆëŠ” ì—…ë¡œë“œ íŒŒì¼ì„ ë””ìŠ¤í¬(uploads í´ë”)ì— ì €ì¥í•©ë‹ˆë‹¤.
        ë™ì¼í•œ íŒŒì¼ëª…ì´ ì—…ë¡œë“œë˜ì–´ë„ ë¬¼ë¦¬ì  ì¶©ëŒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ UUIDë¥¼ ë¶™ì—¬ì„œ ì €ì¥í•©ë‹ˆë‹¤.
        """
        import uuid

        upload_dir = "uploads"
        os.makedirs(upload_dir, exist_ok=True)

        # ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„± (ì˜ˆ: a1b2c3d4..._ë³´ê³ ì„œ.pdf)
        unique_filename = f"{uuid.uuid4()}_{file.filename}"

        # ì €ì¥ë  íŒŒì¼ì˜ ì „ì²´ ì£¼ì†Œ (ì˜ˆ: "uploads/a1b2c3d4..._ë³´ê³ ì„œ.pdf")
        file_path = os.path.join(upload_dir, unique_filename)

        with open(file_path, "wb") as buffer:
            # ë©”ëª¨ë¦¬ì— ìˆëŠ” íŒŒì¼(file.file)ì„ í•˜ë“œë””ìŠ¤í¬(buffer)ë¡œ ë³µì‚¬
            shutil.copyfileobj(file.file, buffer)

        return file_path

    def create_pending_document(
        self,
        knowledge_base_id: UUID,
        filename: str,
        file_path: str | None,
        chunk_size: int,
        chunk_overlap: int,
        source_type: SourceType = SourceType.FILE,
        meta_info: dict = None,
    ) -> UUID:
        """
        íŒŒì¼ ì—…ë¡œë“œ ì‹œì ì— 'Pending' ìƒíƒœì˜ Document ë ˆì½”ë“œë¥¼ ë¨¼ì € ìƒì„±í•©ë‹ˆë‹¤.
        KnowledgeBaseì™€ì˜ ì—°ê²°(FK)ì„ ìœ„í•´ knowledge_base_idê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.
        ì„¤ì •ëœ chunk_sizeì™€ chunk_overlapì„ ì €ì¥í•˜ì—¬ ë‚˜ì¤‘ì— ì°¸ì¡°í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
        """
        new_doc = Document(
            knowledge_base_id=knowledge_base_id,
            filename=filename,
            file_path=file_path,
            status="pending",
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            source_type=source_type,
            meta_info=meta_info or {},
        )
        self.db.add(new_doc)
        self.db.commit()
        self.db.refresh(new_doc)
        return new_doc.id

    async def process_document_background(
        self, document_id: UUID, knowledge_base_id: UUID, file_path: str
    ):
        """
        BackgroundTasksì˜ ë©”ì¸ ì§„ì…ì .
        íŒŒì‹± -> ì²­í‚¹ -> ì„ë² ë”© -> ì €ì¥
        """
        try:
            self._update_status(document_id, "indexing")
            self._update_progress(document_id, 5, "ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            print("[DEBUG] 1ë²ˆ")
            # 1ë‹¨ê³„: íŒŒì‹± (document_id ì „ë‹¬)
            self._update_progress(document_id, 10, "ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")

            # 1.3 DataSourceë¥¼ í†µí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            # DBì—ì„œ Document ê°ì²´ ì¡°íšŒ
            doc = self.db.query(Document).get(document_id)
            if not doc:
                raise ValueError(f"Document {document_id} not found")

            data_source = self._get_data_source(doc.source_type)
            print("[DEBUG] 2ë²ˆ", data_source)

            # ì†ŒìŠ¤ ì„¤ì • êµ¬ì„±
            source_config = {}
            if doc.source_type == SourceType.FILE or str(doc.source_type) == "FILE":
                source_config = {
                    "file_path": file_path,
                    "document_id": str(document_id),
                }
            elif doc.source_type == SourceType.API:
                # meta_infoì—ì„œ API ì„¤ì • ê°€ì ¸ì˜¤ê¸°
                api_config = doc.meta_info.get("api_config", {})

                # í—¤ë” ë³µí˜¸í™” ë¡œì§
                import json

                from core.security import security_service

                headers = api_config.get("headers")
                if headers and isinstance(headers, str):
                    try:
                        decrypted_json = security_service.decrypt(headers)
                        api_config["headers"] = json.loads(decrypted_json)
                    except Exception as e:
                        print(f"Failed to decrypt headers: {e}")
                        # ë³µí˜¸í™” ì‹¤íŒ¨ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©í•˜ê±°ë‚˜ ì—ëŸ¬ ì²˜ë¦¬
                        api_config["headers"] = {}

                source_config = api_config

            text_blocks = data_source.fetch_text(source_config)

            # íŒŒì‹± ê²°ê³¼ê°€ ë¹„ì–´ìˆë‹¤ë©´ (ë¹„ìš© ìŠ¹ì¸ ëŒ€ê¸° ë“±) ì¤‘ë‹¨
            if not text_blocks:
                doc = self.db.query(Document).get(document_id)
                if doc and doc.status == "waiting_for_approval":
                    print(f"â¸ï¸ Document {document_id} paused for approval.")
                    self._update_progress(
                        document_id, 0, "ì¶”ê°€ ë¹„ìš© ìŠ¹ì¸ì´ í•„ìš”í•˜ì—¬ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤."
                    )
                    return
                # ì§„ì§œ ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš°ì¼ ìˆ˜ë„ ìˆìŒ (ì´ ê²½ìš° completed ì²˜ë¦¬ë¨)
                print(f"âš ï¸ No text extracted from document {document_id}")
                self._update_status(document_id, "completed")
                return

            # 1.5 Content Hash Check (ë³€ê²½ ê°ì§€)
            # ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í•©ì³ì„œ í•´ì‹œ ìƒì„±
            full_text = "".join([b["text"] for b in text_blocks])
            new_hash = hashlib.sha256(full_text.encode("utf-8")).hexdigest()

            if doc.content_hash == new_hash:
                print(f"â­ï¸ Content unchanged for {document_id}. Skipping processing.")
                self._update_progress(
                    document_id, 100, "ë³€ê²½ëœ ë‚´ìš©ì´ ì—†ì–´ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤."
                )
                self._update_status(document_id, "completed")
                return

            # í•´ì‹œ ì—…ë°ì´íŠ¸
            doc = self.db.query(Document).get(document_id)
            if doc:
                doc.content_hash = new_hash
                self.db.commit()

            self._update_progress(document_id, 40, "ë¬¸ì„œ ë‚´ìš© ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

            # 2~4ë‹¨ê³„: ì²­í‚¹, ì„ë² ë”©, ì €ì¥ ë° ì™„ë£Œ ì²˜ë¦¬
            self._finalize_ingestion(document_id, knowledge_base_id, text_blocks)
        except Exception as e:
            print(f"Ingestion failed: {e}")
            self._update_status(document_id, "failed", error_message=str(e))
            self._update_progress(
                document_id, 0, f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            )

    async def resume_processing(self, document_id: UUID, strategy: str = "llamaparse"):
        """
        ìŠ¹ì¸ëœ ë¬¸ì„œì— ëŒ€í•´ íŒŒì‹±ì„ ì¬ê°œí•©ë‹ˆë‹¤.
        strategy: 'llamaparse' (ìœ ë£Œ, ê³ í’ˆì§ˆ) or 'general' (ë¬´ë£Œ, PyMuPDF)
        """
        doc = self.db.query(Document).get(document_id)
        if not doc:
            print(f"âŒ Document {document_id} not found for resumption.")
            return

        try:
            print(f"â–¶ï¸ Resuming ingestion for {document_id} with strategy: {strategy}")
            self._update_status(document_id, "indexing")

            text_blocks = []

            # 1ë‹¨ê³„: ì „ëµì— ë”°ë¥¸ íŒŒì‹± (DataSource ì‚¬ìš©)
            data_source = self._get_data_source(doc.source_type)

            source_config = {}
            if doc.source_type == SourceType.FILE:
                source_config = {
                    "file_path": doc.file_path,
                    "document_id": str(document_id),
                    "strategy": strategy,  # "general" or "llamaparse" passed from arg
                }
            elif doc.source_type == SourceType.API:
                api_config = doc.meta_info.get("api_config", {})
                source_config = api_config

            text_blocks = data_source.fetch_text(source_config)

            # 2~4ë‹¨ê³„: ì²­í‚¹, ì„ë² ë”©, ì €ì¥ ë° ì™„ë£Œ ì²˜ë¦¬
            self._finalize_ingestion(document_id, doc.knowledge_base_id, text_blocks)

        except Exception as e:
            print(f"âŒ Resumption failed: {e}")
            self._update_status(document_id, "failed", error_message=str(e))

    def _save_chunks_to_pgvector(
        self, document_id: UUID, knowledge_base_id: UUID, chunks: list[dict]
    ):
        """
        í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ OpenAIì— ë³´ë‚´ì„œ 'ì˜ë¯¸ ë²¡í„°'ë¡œ ë°”ê¾¼ ë’¤, DocumentChunk í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤.
        ê¸°ì¡´ ì²­í¬ê°€ ìˆë‹¤ë©´ ì‚­ì œí•˜ê³  ìƒˆë¡œ ì €ì¥í•©ë‹ˆë‹¤ (Clean & Insert).
        """
        print(f"ğŸ” [Debug] _save_chunks_to_pgvector ì‹œì‘: doc_id={document_id}")
        # 0. ê¸°ì¡´ ì²­í¬ ì‚­ì œ (Clean Step)
        try:
            del_count = (
                self.db.query(DocumentChunk)
                .filter(DocumentChunk.document_id == document_id)
                .delete()
            )
            self.db.commit()
            print(f"ğŸ—‘ï¸ [Debug] ê¸°ì¡´ ì²­í¬ {del_count}ê°œ ì‚­ì œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ [Debug] ê¸°ì¡´ ì²­í¬ ì‚­ì œ ì¤‘ ì—ëŸ¬: {e}")

        # TODO: í† í° ê³„ì‚°ì„ ìœ„í•œ ì¸ì½”ë” ì„¤ì •
        try:
            encoding = tiktoken.encoding_for_model(self.ai_model)
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base")  # gpt-4ë¡œ ê°€ì •í•˜ê³  ê³„ì‚°

        # DBì—ì„œ API Key ê°€ì ¸ì˜¤ê¸° (í™˜ê²½ë³€ìˆ˜ ì˜ì¡´ ì œê±°)
        from db.models.llm import LLMProvider

        api_key = None

        doc = self.db.query(Document).filter(Document.id == document_id).first()
        if not doc:
            print("âŒ [Debug] ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            raise ValueError("ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        user_id = doc.knowledge_base.user_id
        print(f"ğŸ” [Debug] ë¬¸ì„œ ì†Œìœ ì ID: {user_id}")

        user_crd = (
            self.db.query(LLMCredential)
            .join(LLMProvider)
            .filter(
                LLMCredential.user_id == user_id,
                LLMCredential.is_valid,
                LLMProvider.name == "openai",
            )
            .first()
        )

        if user_crd:
            print(f"âœ… [Debug] OpenAI ìê²© ì¦ëª… ë°œê²¬ (ID: {user_crd.id})")
            try:
                config = json.loads(user_crd.encrypted_config)
                api_key = config.get("apiKey")
            except Exception as e:
                print(f"[Debug] Credential config íŒŒì‹± ì‹¤íŒ¨: {e}")

        if not api_key:
            raise ValueError(
                "ì‚¬ìš©ìì˜ OpenAI API Keyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë“±ë¡í•´ì£¼ì„¸ìš”."
            )
            print("âš ï¸ [Debug] OpenAI ìê²© ì¦ëª…ì„ ì°¾ì§€ ëª»í•¨")
        print(f"âœ… [Debug] API Key í™•ë³´ ì™„ë£Œ (Key: {api_key[:8]}...)")

        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (API Key ëª…ì‹œ)
        embeddings_model = OpenAIEmbeddings(model=self.ai_model, openai_api_key=api_key)

        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = [chunk["content"] for chunk in chunks]
        print(f"ğŸ” [Debug] ì„ë² ë”© ìš”ì²­ ì‹œì‘ (ì²­í¬ ê°œìˆ˜: {len(texts)}ê°œ)")

        # 2. ì„ë² ë”© ìƒì„± (ì¼ê´„ í˜¸ì¶œ) - ì‹¤ì œ API ì‚¬ìš©!
        try:
            embedded_vectors = embeddings_model.embed_documents(texts)
            print("âœ… [Debug] ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ [Debug] OpenAI Embedding Error: {e}")
            raise e

        # 3. DB ê°ì²´ ìƒì„±
        try:
            chunk_objects = []
            for i, chunk in enumerate(chunks):
                content = chunk["content"]
                token_count = len(encoding.encode(content))

                db_chunk = DocumentChunk(
                    document_id=document_id,
                    knowledge_base_id=knowledge_base_id,  # ê²€ìƒ‰ ìµœì í™”ìš©
                    content=content,
                    embedding=embedded_vectors[i],
                    chunk_index=i,
                    token_count=token_count,
                    metadata_=chunk["metadata"],
                )
                chunk_objects.append(db_chunk)

            print(
                f"ğŸ“¦ [Debug] ì €ì¥í•  ê°ì²´ {len(chunk_objects)}ê°œ ìƒì„±ë¨. DBì— ì¶”ê°€(add) ì‹œë„..."
            )
            self.db.add_all(chunk_objects)
            print("ğŸ’¾ [Debug] ì»¤ë°‹(Commit) ì‹œë„...")
            self.db.commit()
            print("ğŸ‰ [Debug] DB ì €ì¥ ë° ì»¤ë°‹ ì„±ê³µ!")

        except Exception as e:
            print(f"âŒ [Debug] DB ì €ì¥ ì‹¤íŒ¨ (Commit Error): {e}")
            self.db.rollback()  # ë¡¤ë°± ì‹œë„
            raise e

    def _create_chunks(self, text_blocks: list[dict]) -> list[dict]:
        """
        í…ìŠ¤íŠ¸ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„, ì„¤ì •ëœ chunk_sizeì™€ chunk_overlapì— ë”°ë¼ ì²­í‚¹í•©ë‹ˆë‹¤.
        ê° ì²­í¬ëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ ë¸”ë¡ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ë³‘í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        chunks = []
        for block in text_blocks:
            text = block["text"]
            metadata = block.get("metadata", {})

            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µí•  ìˆ˜ë„ ìˆìŒ (ì„ íƒì‚¬í•­)
            if not text.strip():
                continue

            splits = self.text_splitter.split_text(text)

            for split in splits:
                chunks.append(
                    {
                        "content": split,
                        "metadata": metadata,  # í˜ì´ì§€ ë²ˆí˜¸ ë“± ì›ë³¸ ë©”íƒ€ë°ì´í„° ë³´ì¡´
                    }
                )

        return chunks

    def _finalize_ingestion(
        self, document_id: UUID, knowledge_base_id: UUID, text_blocks: list[dict]
    ):
        """
        í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ë°›ì•„ ì²­í‚¹ -> ì„ë² ë”© -> ì €ì¥ -> ì™„ë£Œ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        # 2ë‹¨ê³„: ì²­í‚¹
        self._update_progress(
            document_id, 50, "AIê°€ ì½ê¸° ì¢‹ê²Œ ë¬¸ì„œë¥¼ ì¡°ê°ë‚´ê³  ìˆìŠµë‹ˆë‹¤..."
        )
        chunks = self._create_chunks(text_blocks)

        # 3 & 4ë‹¨ê³„: ì„ë² ë”© ë° ì €ì¥
        self._update_progress(
            document_id, 70, "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•  ì¤€ë¹„ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
        )
        self._save_chunks_to_pgvector(document_id, knowledge_base_id, chunks)

        # ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        self._update_progress(document_id, 100, "ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        self._update_status(document_id, "completed")

    def _update_status(self, document_id: UUID, status: str, error_message: str = None):
        doc = self.db.query(Document).get(document_id)
        if doc:
            doc.status = status
            if error_message:
                doc.error_message = error_message
            self.db.commit()

    def _update_progress(self, document_id: UUID, progress: int, message: str):
        """
        ë¬¸ì„œ ì²˜ë¦¬ ì§„í–‰ë¥ (%)ê³¼ í˜„ì¬ ë‹¨ê³„ ë©”ì‹œì§€ë¥¼ meta_infoì— ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        """
        doc = self.db.query(Document).get(document_id)
        if doc:
            new_meta = dict(doc.meta_info or {})
            new_meta.update(
                {"processing_progress": progress, "processing_current_step": message}
            )
            doc.meta_info = new_meta
            self.db.commit()

    async def analyze_document(self, document_id: UUID) -> dict:
        """
        ë¬¸ì„œ ë¶„ì„: í˜ì´ì§€ ìˆ˜, ë¹„ìš© ì˜ˆì¸¡ ë“±ì„ ë°˜í™˜
        """
        doc = self.db.query(Document).get(document_id)
        if not doc:
            raise ValueError("Document not found")

        # 1. ë¹„ìš© ì˜ˆì¸¡ (FileDataSource ì‚¬ìš©)
        try:
            # ì„ì‹œë¡œ FILE íƒ€ì… ê°€ì • (API ë“±ì€ 0 ë°˜í™˜)
            data_source = self._get_data_source(doc.source_type)
            source_config = {}
            if doc.source_type == SourceType.FILE:
                source_config = {"file_path": doc.file_path}

            cost_info = data_source.estimate_cost(source_config)
        except Exception as e:
            print(f"Cost estimation failed: {e}")
            cost_info = {"pages": 0, "credits": 0, "cost_usd": 0.0}

        # 2. íŒŒì¼ íƒ€ì… ë¶„ì„ (ì„ íƒ ì‚¬í•­)
        # parsing_strategy = self._analyze_pdf_type(doc.file_path)

        # 3. ìºì‹œ í™•ì¸ (íŒŒì¼ì¸ ê²½ìš°ì—ë§Œ)
        is_cached = False
        cache_path = ""

        if doc.source_type == SourceType.FILE and doc.file_path:
            cache_path = self._get_cache_path(doc.file_path)
            is_cached = os.path.exists(cache_path)

        print(
            f"ğŸ” [Debug] analyze_document: filename={doc.filename}, is_cached={is_cached}, path={cache_path}"
        )

        return {
            "cost_estimate": cost_info,
            "filename": doc.filename,
            "is_cached": is_cached,
            # "recommended_strategy": parsing_strategy
        }

    def _get_cache_path(self, file_path: str) -> str:
        """
        LlamaParse ê²°ê³¼ ìºì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        (ì˜ˆ: uploads/file.pdf -> uploads/file.pdf.md)
        """
        if not file_path:
            return ""
        return f"{file_path}.md"

    def preview_chunking(
        self,
        file_path: str,
        chunk_size: int,
        chunk_overlap: int,
        segment_identifier: str,
        remove_urls_emails: bool = False,
        remove_whitespace: bool = True,
        strategy: str = "general",  # "general" or "llamaparse",
        source_type: SourceType = SourceType.FILE,
        meta_info: dict = None,
    ) -> list[dict]:
        """
        DB ì €ì¥ ì—†ì´ ë©”ëª¨ë¦¬ ìƒì—ì„œ ì²­í‚¹ ê²°ê³¼ë¥¼ ë¯¸ë¦¬ë´…ë‹ˆë‹¤.
        strategyì— ë”°ë¼ ì¼ë°˜ íŒŒì‹± ë˜ëŠ” ì •ë°€ íŒŒì‹±(LlamaParse)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        try:
            if source_type == SourceType.API:
                # API ë°˜í™˜ê°’ ì²˜ë¦¬, í—¤ë” ë³µí˜¸í™”
                api_config = meta_info.get("api_config", {})
                headers = api_config.get("headers")
                if headers and isinstance(headers, str):
                    try:
                        from core.security import security_service

                        decrypted_json = security_service.decrypt(headers)
                        api_config["headers"] = json.loads(decrypted_json)
                    except Exception as e:
                        print(f"Failed to decrypt headers: {e}")
                        api_config["headers"] = {}

                data_source = ApiDataSource()
                source_config = api_config
            else:
                data_source = FileDataSource()
                source_config = {
                    "file_path": file_path,
                    "strategy": strategy,  # "general" or "llamaparse"
                }

            text_blocks = data_source.fetch_text(source_config)
            full_text = "\n".join([block["text"] for block in text_blocks])
        except Exception as e:
            print(f"Preview parsing failed: {e}")
            return []

        # 2. ì „ì²˜ë¦¬
        if remove_urls_emails:
            # URL ì œê±°
            full_text = re.sub(
                r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+",
                "",
                full_text,
            )
            # ì´ë©”ì¼ ì œê±°
            full_text = re.sub(r"[\w\.-]+@[\w\.-]+", "", full_text)

        if remove_whitespace:
            # ì—°ì†ëœ ê³µë°±, íƒ­ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
            full_text = re.sub(r"[ \t]+", " ", full_text)
            # ì—°ì†ëœ ì¤„ë°”ê¿ˆì´ 3ê°œ ì´ìƒì´ë©´ 2ê°œ(\n\n)ë¡œ ì¶•ì†Œ (ë¬¸ë‹¨ êµ¬ë¶„ ìœ ì§€)
            full_text = re.sub(r"\n{3,}", "\n\n", full_text)

        # 3. ì²­í‚¹ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
        # segment_identifierê°€ ìœ íš¨í•˜ë©´ separator ëª©ë¡ì˜ ìµœìš°ì„  ìˆœìœ„ë¡œ ì¶”ê°€
        separators = ["\n\n", "\n", ".", " ", ""]
        if segment_identifier and segment_identifier not in separators:
            # íŠ¹ìˆ˜ ë¬¸ì(escaped) ì²˜ë¦¬ í•„ìš”í•  ìˆ˜ ìˆìŒ. ì¼ë‹¨ ìˆëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©.
            # ì‚¬ìš©ìê°€ "\n\n"ì„ ì…ë ¥í•˜ë©´ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ë“¤ì–´ì˜¤ë¯€ë¡œ, ì‹¤ì œ ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë¡œì§ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ.
            # í”„ë¡ íŠ¸ì—ì„œ ì‹¤ì œ ì¤„ë°”ê¿ˆì„ ë³´ë‚´ê±°ë‚˜, ì—¬ê¸°ì„œ ë³€í™˜í•´ì•¼ í•¨.
            # ì¼ë‹¨ì€ ë‹¨ìˆœ ë¬¸ìì—´ ë§¤ì¹­ìœ¼ë¡œ ê°€ì •í•˜ë˜, \nì€ íŠ¹ë³„ ì·¨ê¸‰
            processed_identifier = segment_identifier.replace("\\n", "\n")
            if processed_identifier not in separators:
                separators.insert(0, processed_identifier)

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=separators,
            keep_separator=True,
        )

        splits = splitter.split_text(full_text)

        # 4. ê²°ê³¼ í¬ë§·íŒ… & í† í° ê³„ì‚°
        try:
            encoding = tiktoken.encoding_for_model(self.ai_model)
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base")

        preview_segments = []
        for split in splits:
            token_count = len(encoding.encode(split))
            preview_segments.append(
                {
                    "content": split,
                    "token_count": token_count,
                    "char_count": len(split),
                }
            )

        return preview_segments
